{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437f162e",
   "metadata": {},
   "source": [
    "#### Problem Statement\n",
    "\n",
    "##### Project 10\n",
    "##### Intensity Analysis (Build your own model using NLP and Python) \n",
    "\n",
    "The objective of this project is to develop an intelligent system using NLP to predict the intensity in the text reviews. By analyzing various parameters and process data, the system will predict the intensity where its happiness, angriness or sadness. This predictive capability will enable to proactively optimize their processes, and improve overall customer satisfaction.\n",
    "\n",
    "\n",
    "#### Purpose of the solving this problem\n",
    "\n",
    "The purpose of developing an intelligent system using NLP to predict the intensity in text reviews can serve multiple practical applications, including:\n",
    "\n",
    "##### 1. Enhanced Customer Feedback Analysis\n",
    "\n",
    "Understanding Emotion and Sentiment: Predicting intensity helps companies understand not just whether a review is positive or negative but also the strength of the sentiment. This is useful for prioritizing urgent customer issues (e.g., highly negative reviews) or identifying highly satisfied customers for engagement.\n",
    "\n",
    "Improving Customer Service: By assessing the intensity of reviews, companies can automatically flag highly intense negative reviews for faster resolution, helping to improve customer retention and overall satisfaction.\n",
    "\n",
    "##### 2. Improved Product Development\n",
    "\n",
    "Prioritizing Feedback: Companies can focus on features or products that evoke strong positive or negative reactions. Reviews with higher emotional intensity might signal critical areas to focus on for improvement or enhancement.\n",
    "\n",
    "Product and Service Customization: Understanding how strongly customers feel about certain aspects of a product can guide product customization to align better with customer preferences.\n",
    "\n",
    "##### 3. Automated Review Moderation\n",
    "\n",
    "Identifying Harmful or Fake Reviews: Reviews with extreme or unusual levels of intensity might indicate spam, fake, or abusive reviews. The system can help automatically flag these reviews for further investigation or filtering.\n",
    "\n",
    "Filtering Content: Intensity prediction can aid in automating moderation by detecting emotionally charged or inappropriate content in public forums or review platforms.\n",
    "\n",
    "##### 4. Market Sentiment and Trend Analysis\n",
    "\n",
    "Tracking Sentiment Over Time: Predicting intensity can help businesses understand trends in customer sentiment, monitoring changes in customer satisfaction or dissatisfaction over time.\n",
    "\n",
    "Competitive Analysis: Companies can analyze the intensity of reviews for their competitors’ products, gaining insight into how strongly customers react to competitor offerings, which can influence strategic decisions.\n",
    "\n",
    "##### 5. Mental Health and Well-being Insights\n",
    "\n",
    "Detecting Emotional Distress: In certain applications, such as mental health support platforms, intensity prediction can help detect emotional distress in written feedback or reviews, leading to timely intervention.\n",
    "\n",
    "Empathy and Interaction: Intelligent systems can be designed to respond with an appropriate tone based on the predicted emotional intensity, enhancing user interaction in support or conversational agents.\n",
    "\n",
    "In summary, the purpose of solving this problem is to enable deeper insight into the emotional or sentiment-driven aspects of text reviews. This can empower businesses and platforms to enhance customer experience, refine products, manage reputation, and respond more effectively to user feedback.\n",
    "\n",
    "#### Dataset Information\n",
    "\n",
    "The dataset for this project can be accessed by clicking the link provided below\n",
    "\n",
    "https://kh3-ls-storage.s3.us-east-1.amazonaws.com/Updated Project guide data set/Intensity_data.zip\n",
    "\n",
    "This data set includes 3 csv files named angriness.csv, happiness.csv and sadness.csv each conatining 2 columns namely \n",
    "\n",
    "content : Contains the customer reviews\n",
    "\n",
    "intensity : Describes the sentiment in the review which is angriness in angriness.csv, sadness in sadness.csv, happiness in happiness.csv\n",
    "\n",
    "#### Instructions on how to run this project code on your system\n",
    "\n",
    "Step 1 : Download this project repository as a zip file.\n",
    "Step 2 : Unzip the folder to your desired location\n",
    "Step 3 : Go to the Anaconda website and download the installer for your operating system (Windows, macOS, or Linux) & launch it.\n",
    "Step 4 : Launch Jupyter Notebook Interface from Anaconda Navigator after whic, it opens in your default browser.\n",
    "Step 5 : Navigate to this project folder.\n",
    "Step 6 : When inside navigate to intensityanalysis > notebooks > intensityanalysis.pynb\n",
    "Step 7 : Open the intensityanalysis.pynb\n",
    "Step 8 : Replace the filepaths to store the data, models and visuals in the directory where you have unzipped the project folder\n",
    "Step 9 : Save and Run the intensityanalysis.pynb file.\n",
    "Step 10 : Wait for the file to complete executing the program and then check the output along with the contents in the data, models and visuals directories.\n",
    "\n",
    "\n",
    "#### Explanations of code and models\n",
    "\n",
    "##### The comments mentioned throughout the notebook already explain the code and the models. However for the purpose of detailed understanding here is the detailed explanation.\n",
    "\n",
    "#### A) Loading the dataset, performing Exploratory Data Analysis and Feature Engineering\n",
    "\n",
    "##### I) Pandas Cloumn Configuration \n",
    "\n",
    "The code pd.set_option('display.max_colwidth', None) is a pandas configuration setting that changes the way DataFrame columns are displayed in the console or notebook.\n",
    "\n",
    "pd.set_option(): This is a function from pandas that allows you to modify certain display options.\n",
    "'display.max_colwidth': This option controls the maximum width (in characters) for the display of each column in a DataFrame when printed.\n",
    "\n",
    "None: Setting it to None means there is no limit on the width. The full content of a column will be displayed, no matter how long the values are.\n",
    "\n",
    "Purpose:\n",
    "By default, pandas truncates the content of cells if it exceeds a certain length, showing an ellipsis (...). By setting max_colwidth to None, you can ensure that the entire content of each cell is displayed without truncation.\n",
    "\n",
    "##### II) Loading the datasets and checking them\n",
    "\n",
    "df1 = pd.read_csv(\"C:/Users/nikde/Documents/UpGrad/intensityanalysis/data/rawdata/Intensity_data/angriness.csv\")\n",
    "Loads angriness.csv into df1\n",
    "\n",
    "df1.head() - Shows the first 5 rows of df1\n",
    "\n",
    "df1.shape - Shows the shape of df1\n",
    "\n",
    "df1.duplicated().sum() - checks the total duplicates in df1\n",
    "\n",
    "df2 = pd.read_csv(\"C:/Users/nikde/Documents/UpGrad/intensityanalysis/data/rawdata/Intensity_data/happiness.csv\")\n",
    "Loads happiness.csv into df1\n",
    "\n",
    "df2.head() - Shows the first 5 rows of df2\n",
    "\n",
    "df2.shape - Shows the shape of df2\n",
    "\n",
    "df2.duplicated().sum() - checks the total duplicates in df2\n",
    "\n",
    "df3 = pd.read_csv(\"C:/Users/nikde/Documents/UpGrad/intensityanalysis/data/rawdata/Intensity_data/sadness.csv\")\n",
    "Loads sadness.csv into df3\n",
    "\n",
    "df3.head() - Shows the first 5 rows of df3\n",
    "\n",
    "df3.shape - Shows the shape of df3\n",
    "\n",
    "df3.duplicated().sum() - checks the total duplicates in df3\n",
    "\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True) - Appends the datasets (concatenates them row-wise)\n",
    "\n",
    "##### III) Specify File Paths and Save DataFrame\n",
    "\n",
    "file_path = os.path.join(\"C:/Users/nikde/Documents/UpGrad/intensityanalysis/data/processeddata\", \"original_appended_data.csv\")\n",
    "Specifies the file path where you want to save the file\n",
    "\n",
    "df.to_csv(file_path, index=False)\n",
    "Saves the df dataframe as a CSV file and index=False to helps to avoid saving the DataFrame index\n",
    "\n",
    "df.head() - Shows the first 5 rows of df\n",
    "\n",
    "df.shape - Shows the shape of df\n",
    "\n",
    "df.duplicated().sum() - checks the total duplicates in df\n",
    "\n",
    "##### III) Explorartory Data Analysis\n",
    "\n",
    "df.info()\n",
    "This provides a concise summary of the df DataFrame, including the number of entries, column names, data types, and non-null counts.\n",
    "\n",
    "df.isna().sum()/len(df) * 100\n",
    "This line calculates the percentage of missing (NaN) values in each column of the DataFrame df.\n",
    "\n",
    "df.isna().sum() counts the number of missing values per column.\n",
    "\n",
    "Dividing by len(df) (the total number of rows) and multiplying by 100 gives the percentage of missing values for each column.\n",
    "\n",
    "df.duplicated().sum() - checks the total duplicates in df\n",
    "\n",
    "visuals_folder = \"C:/Users/nikde/Documents/UpGrad/intensityanalysis/visuals\"\n",
    "Defines a folder to save visualizations\n",
    "\n",
    "visuals_folder_aa = \"C:/Users/nikde/Documents/UpGrad/intensityanalysis/visuals/afteraugmentation\"\n",
    "Defines a folder to save augmented visualizations\n",
    "\n",
    "##### Plotting the target class distribution:\n",
    "\n",
    "label_dist = df['intensity'].value_counts().to_dict()\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "ax = plt.bar(label_dist.keys(), label_dist.values(), width=0.25)\n",
    "plt.xticks([0,1,2])\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Content Count\")\n",
    "plt.savefig(os.path.join(visuals_folder, \"initial_sentiment_distribution.png\"))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df['intensity'].value_counts(): Counts the occurrences of each unique value in the 'intensity' column, which is assumed to be the target or sentiment label (e.g., 0, 1, 2 representing different sentiment categories).\n",
    "\n",
    ".to_dict(): Converts the resulting counts into a dictionary where the keys are the unique intensity labels, and the values are their corresponding counts.\n",
    "\n",
    "plt.figure(figsize=(10, 5)): Creates a new figure for the plot with a specified width (10) and height (5).\n",
    "\n",
    "plt.bar(): Generates a bar plot where the x-axis is the intensity labels (keys of the label_dist dictionary), and the y-axis is their respective counts (values of the dictionary).\n",
    "\n",
    "width=0.25: Sets the width of each bar in the plot.\n",
    "\n",
    "plt.xticks([0,1,2]): Specifies the tick labels on the x-axis, which correspond to the sentiment categories (0, 1, 2).\n",
    "\n",
    "plt.xlabel(\"Sentiment\"): Labels the x-axis as \"Sentiment\".\n",
    "\n",
    "plt.ylabel(\"Content Count\"): Labels the y-axis as \"Content Count\".\n",
    "\n",
    "plt.savefig(): Saves the generated plot as a PNG image to the specified file path (visuals_folder), with the file name initial_sentiment_distribution.png.\n",
    "\n",
    "plt.show(): Displays the plot on the screen.\n",
    "\n",
    "##### \n",
    "\n",
    "df['length']=df['content'].apply(lambda x: len(x.split(' ')))\n",
    "adds a new column for the length of the reveiws stored in df['content']\n",
    "\n",
    "df.head(10) - Shows the first 10 rows of df\n",
    "\n",
    "print(round(df[df['intensity']=='angriness']['length'].mean()))\n",
    "print(round(df[df['intensity']=='happiness']['length'].mean()))\n",
    "print(round(df[df['intensity']=='sadness']['length'].mean()))\n",
    "\n",
    "prints the mean length for angriness, happiness and sadness reveiews\n",
    "\n",
    "##### Plotting the distribution based on the length of the reveiews for each intensity\n",
    "\n",
    "df[df['intensity']=='angriness']['length'].plot.hist(bins=15, alpha=0.3, label=\"angriness\")\n",
    "df[df['intensity']=='happiness']['length'].plot.hist(bins=15, alpha=1, label=\"happiness\")\n",
    "df[df['intensity']=='sadness']['length'].plot.hist(bins=15, alpha=0.3, label=\"sadness\")\n",
    "plt.xlabel(\"length\")\n",
    "plt.savefig(os.path.join(visuals_folder, \"initial_review_length_distribution.png\"))\n",
    "plt.show()\n",
    "\n",
    "df[df['intensity']=='angriness']: Filters the DataFrame to include only the rows where the 'intensity' column is equal to 'angriness'.\n",
    "\n",
    "['length']: Selects the 'length' column from the filtered DataFrame, which presumably contains the lengths of text reviews.\n",
    "\n",
    ".plot.hist(bins=15, alpha=0.3, label=\"angriness\"): Creates a histogram for the review lengths of the 'angriness' category:\n",
    "\n",
    "bins=15: Specifies the number of bins to divide the range of lengths into, allowing for a clearer view of the distribution.\n",
    "\n",
    "alpha=0.3: Sets the transparency of the bars to 30%, allowing overlapping bars from different categories to be seen.\n",
    "\n",
    "label=\"angriness\": Labels this histogram for the legend.\n",
    "\n",
    "df[df['intensity']=='happiness']['length'].plot.hist(bins=15, alpha=1, label=\"happiness\")\n",
    "This line performs the same operation as the previous one but for the 'happiness' category. Here, alpha=1 means the bars will be fully opaque.\n",
    "\n",
    "df[df['intensity']=='sadness']['length'].plot.hist(bins=15, alpha=0.3, label=\"sadness\")\n",
    "Similar to the previous lines, this one creates a histogram for the 'sadness' category, with a transparency of 30%.\n",
    "\n",
    "plt.xlabel(\"length\")\n",
    "This line labels the x-axis as \"length\", indicating that the x-axis represents the lengths of the text reviews.\n",
    "\n",
    "plt.savefig(): Saves the histogram plot as a PNG image in the specified directory (visuals_folder) with the filename initial_review_length_distribution.png.\n",
    "\n",
    "plt.show(): Displays the plot on the screen.\n",
    "\n",
    "#####\n",
    "\n",
    " STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "stopwords.words('english'): This function call comes from the nltk library (Natural Language Toolkit). It retrieves a list of English stopwords, which are words that typically carry little meaning and are usually removed in text preprocessing. Common examples of stopwords include words like \"the,\" \"is,\" \"in,\" \"and,\" etc.\n",
    "\n",
    "set(...): The set function converts the list of stopwords into a set. Using a set is beneficial because:\n",
    "\n",
    "Faster Lookup: Sets provide faster membership testing compared to lists. This means checking if a word is a stopword will be quicker.\n",
    "\n",
    "No Duplicates: A set automatically eliminates any duplicate entries, ensuring that each stopword is only represented once.\n",
    "\n",
    "Purpose:\n",
    "\n",
    "The purpose of creating the STOPWORDS set is to prepare for text preprocessing tasks, such as tokenization or text classification, where you may want to exclude these common words to focus on more meaningful words in the text.\n",
    "\n",
    "#####\n",
    "\n",
    " df[\"clean_content\"]=df[\"content\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (STOPWORDS)]))\n",
    "\n",
    "df[\"content\"]: This references the \"content\" column in the DataFrame df, which presumably contains raw text data (e.g., user reviews, comments, etc.).\n",
    "\n",
    ".apply(...): The apply function is used to apply a function to each element of the Series (in this case, each row in the \"content\" column).\n",
    "\n",
    "lambda x: ...: This defines an anonymous (lambda) function that takes an input x, representing a single piece of text from the \"content\" column.\n",
    "\n",
    "x.split(): This method splits the string x into a list of words (tokens) based on whitespace. For example, the sentence \"This is a test.\" would become [\"This\", \"is\", \"a\", \"test.\"].\n",
    "\n",
    "[word for word in x.split() if word not in STOPWORDS]: This list comprehension iterates over each word in the list created by x.split(). It constructs a new list that includes only the words that are not in the STOPWORDS set. Essentially, it filters out any stopwords.\n",
    "\n",
    "' '.join(...): After filtering, the join method concatenates the remaining words back into a single string, with a space ' ' between each word.\n",
    "\n",
    "df[\"clean_content\"] = ...: The resulting cleaned text (with stopwords removed) is assigned to a new column named \"clean_content\" in the DataFrame df.\n",
    "\n",
    "Purpose:\n",
    "The purpose of this code is to preprocess the text data in the \"content\" column by removing common, less meaningful words (stopwords). The cleaned text is stored in a new column (\"clean_content\"), which can be used for further analysis or modeling tasks, such as sentiment analysis or text classification.\n",
    "\n",
    "#####\n",
    "\n",
    "df.head(10) - Shows the first 10 rows of df again\n",
    "\n",
    "##### defining a function for content cleaning\n",
    "\n",
    "def text_clean(content):\n",
    "    #lowercase the content\n",
    "    content=content.lower()\n",
    "    #remove punctuation\n",
    "    content = re.sub('[()!?]', ' ', content)\n",
    "    content = re.sub('\\[.*?\\]',' ', content)\n",
    "    #remove non alphanumeric occurences\n",
    "    content = re.sub(\"[^a-z0-9]\",\" \", content)\n",
    "    #remove the @mention\n",
    "    content = re.sub(\"@[A-Za-z0-9_]+\",\"\", content)\n",
    "    #remove the hashtags\n",
    "    content = re.sub(\"#[A-Za-z0-9_]+\",\"\", content)\n",
    "    #remove any links\n",
    "    content = re.sub(r\"http\\S+\", \"\", content)\n",
    "    content = re.sub(r\"www.\\S+\", \"\", content)\n",
    "    return content\n",
    "    \n",
    "Lowercase the content: Converts all characters in the text to lowercase to ensure uniformity. This helps avoid case-sensitive mismatches during further processing (e.g., 'Hello' and 'hello' will be treated as the same word).\n",
    "\n",
    "Remove specific punctuation: The first re.sub removes parentheses (), exclamation marks !, question marks ?, and replaces them with spaces.The second re.sub removes any text inside square brackets [ ], along with the brackets themselves.\n",
    "\n",
    "Remove non-alphanumeric characters: This re.sub replaces any character that is not a lowercase letter (a-z) or a digit (0-9) with a space. This effectively removes punctuation and special characters while keeping words and numbers intact.\n",
    "\n",
    "Remove mentions: This line removes any Twitter-like mentions (e.g., @username) from the text.\n",
    "\n",
    "Remove hashtags: Similar to mentions, this line removes hashtags (e.g., #hashtag) from the text.\n",
    "\n",
    "Remove links: The first re.sub removes any URLs that start with http, while the second removes URLs that start with www..\n",
    "python\n",
    "\n",
    "Return the cleaned content: Finally, the function returns the cleaned version of the text.\n",
    "\n",
    "#####\n",
    "\n",
    "df['clean_content'] = df['clean_content'].apply(text_clean)\n",
    "applies the function on the clean_content column\n",
    "\n",
    "df.head(10) - Shows the first 10 rows of df after cleaning\n",
    "\n",
    "df = df.drop('length', axis = 1) - drops the length column\n",
    "\n",
    "df.head() - checks df for the change\n",
    "\n",
    "df['length']=df['clean_content'].apply(lambda x: len(x.split(' '))) \n",
    "adds back the length column after the clean_content column for the length of the cleaned reveiws\n",
    "\n",
    "df.head(5) - checks df for the change\n",
    "\n",
    "df['clean_content'] - shows the contents of the clean_content column\n",
    "\n",
    "df.head() - inspects the clean_content column\n",
    "\n",
    "df[\"clean_content\"]=df[\"clean_content\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (STOPWORDS)]))\n",
    "\n",
    "removes the stop words again because of new words created after applying the text_clean function\n",
    "\n",
    "df[\"clean_content\"] - shows the contents of the clean_content column again\n",
    "\n",
    "df.head() - checks df again after removing stopwords one more time\n",
    "\n",
    "df['length']=df['clean_content'].apply(lambda x: len(x.split(' '))) \n",
    "updates the length column after removing stopwords again for the length of the cleaned reveiws\n",
    "\n",
    "df.head() - checks df again for the length of the cleaned reviews\n",
    "\n",
    "df['clean_content'] = df['clean_content'].apply(text_clean) \n",
    "applies the text clean method again on the clean_content column \n",
    "\n",
    "df['clean_content'] - shows the contents of the clean_content column again\n",
    "\n",
    "df.head() - inspects the first 5 rows of df\n",
    "\n",
    "df['length']=df['clean_content'].apply(lambda x: len(x.split(' ')))\n",
    "updates the length of the reveiws after removing stopwords and applying the text_clean method\n",
    "\n",
    "df.head() - inspects the first 5 rows of df lastly\n",
    "\n",
    "file_path = os.path.join(\"C:/Users/nikde/Documents/UpGrad/intensityanalysis/data/processeddata\", \"initial_feature_engineered_data.csv\")\n",
    "Specifies the file path where you want to save the file\n",
    "\n",
    "df.to_csv(file_path, index=False)  \n",
    "Saves the df dataframe as a CSV file after feature engineering\n",
    "Sets the index=False to avoid saving the DataFrame index\n",
    "    \n",
    "##### Displaying the top 50 words by frequency\n",
    "\n",
    "top_words = 50\n",
    "Setting the number of top words: This variable defines how many of the most common words you want to extract from the dataset.\n",
    "\n",
    "words = nltk.tokenize.word_tokenize(df['clean_content'].str.cat(sep=' '))\n",
    "\n",
    "Tokenizing the cleaned content:\n",
    "df['clean_content'].str.cat(sep=' '): This part concatenates all the entries in the clean_content column of the DataFrame df into a single string, separating each entry with a space.\n",
    "\n",
    "nltk.tokenize.word_tokenize(...): The word_tokenize function from the Natural Language Toolkit (NLTK) is then used to split this concatenated string into individual words (tokens). This results in a list of words from all the cleaned text in the DataFrame.\n",
    "\n",
    "filter_words = [word for word in words if word not in STOPWORDS]\n",
    "\n",
    "Filtering out stopwords:\n",
    "\n",
    "This line uses a list comprehension to iterate over the list of words.\n",
    "\n",
    "For each word, it checks whether it is not in the predefined STOPWORDS set (which typically includes common words like \"the\", \"is\", \"in\", etc., that do not carry significant meaning in text analysis).\n",
    "\n",
    "Only words that are not in the stopwords list are included in the filter_words list.\n",
    "\n",
    "word_freq = nltk.FreqDist(filter_words)\n",
    "\n",
    "nltk.FreqDist(...): This function from the NLTK (Natural Language Toolkit) library creates a frequency distribution of the given input. In this case, the input is filter_words, which is a list of words that have already been filtered to exclude common stopwords.\n",
    "\n",
    "filter_words: This list contains significant words from the cleaned text data after removing stopwords.\n",
    "\n",
    "Purpose\n",
    "The purpose of creating a frequency distribution is to count how many times each word appears in the filter_words list. The result, word_freq, will be a dictionary-like object where:\n",
    "\n",
    "The keys are the unique words from filter_words.\n",
    "The values are the counts of how many times each word appears.\n",
    "\n",
    "wordfreq_df = pd.DataFrame(word_freq.most_common(top_words), columns=['Word', 'Frequency'])\n",
    "\n",
    "word_freq.most_common(top_words):\n",
    "\n",
    "This method retrieves the top_words most common words from the word_freq frequency distribution. It returns a list of tuples, where each tuple consists of a word and its corresponding frequency count.\n",
    "                    \n",
    "pd.DataFrame(...):\n",
    "\n",
    "This part of the code uses pandas to create a DataFrame from the list of tuples generated by most_common().\n",
    "The DataFrame is structured in two columns: one for the words ('Word') and one for their frequencies ('Frequency').\n",
    "columns=['Word', 'Frequency']:\n",
    "\n",
    "This argument specifies the names of the columns in the resulting DataFrame, making it clear what data each column represents.                    \n",
    "The purpose of this code is to organize the frequency data into a structured format that can be easily analyzed, manipulated, or visualized. A DataFrame is a convenient way to hold tabular data, and it allows for easy access to specific rows, filtering, sorting, and more.\n",
    "\n",
    "wordfreq_df - displays the wordfreq_df dataframe\n",
    "\n",
    "##### \n",
    "\n",
    "file_path = os.path.join(\"C:/Users/nikde/Documents/UpGrad/intensityanalysis/data/processeddata\",\"initial_wordfrequency_data.csv\")\n",
    "\n",
    "Specifies the file path where you want to save the file\n",
    "\n",
    "wordfreq_df.to_csv(file_path, index=False)  \n",
    "Save the wordfreq_df dataframe as a CSV file\n",
    "Set index=False to avoid saving the DataFrame index\n",
    "\n",
    "print(round(df[df['intensity']=='angriness']['length'].mean()))\n",
    "print(round(df[df['intensity']=='happiness']['length'].mean()))\n",
    "print(round(df[df['intensity']=='sadness']['length'].mean()))\n",
    "\n",
    "prints mean length for angriness, happiness and sadness reveiews after cleaning\n",
    "\n",
    "##### Plotting the distribution based on the length of the reveiews for each intensity after cleaning\n",
    "\n",
    "The explanation is same as in the previous plot except\n",
    "\n",
    "plt.savefig(os.path.join(visuals_folder, \"reveiw_len_dist_after_feature_engineering.png\")) \n",
    "Saves the histogram plot as a PNG image in the specified directory (visuals_folder) with the filename reveiw_len_dist_after_feature_engineering.png\n",
    "\n",
    "#### B) Performing Train Test Split¶\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"clean_content\"],\n",
    "                                                    df[\"intensity\"],test_size=0.2,\n",
    "                                                    stratify=df['intensity'])\n",
    "\n",
    "train_test_split(...):\n",
    "\n",
    "This function is part of the sklearn.model_selection module and is used to split arrays or matrices into random train and test subsets.\n",
    "In this case, it splits the DataFrame df into training and testing sets.\n",
    "Parameters:\n",
    "\n",
    "df[\"clean_content\"]: This is the feature variable (or independent variable) that will be used for training the model. It contains the cleaned text data.\n",
    "\n",
    "df[\"intensity\"]: This is the target variable (or dependent variable) that the model will predict. It contains the intensity labels associated with each piece of text.\n",
    "\n",
    "test_size=0.2: This specifies that 20% of the data should be set aside for testing. The remaining 80% will be used for training the model.\n",
    "\n",
    "stratify=df['intensity']: This parameter ensures that the split maintains the proportion of each class in the target variable. If, for example, 30% of the data belongs to class A and 70% belongs to class B, the training and testing sets will also reflect this distribution. This is particularly useful when dealing with imbalanced datasets.\n",
    "Output:\n",
    "\n",
    "X_train: This variable will contain the training set of the cleaned text content.\n",
    "X_test: This variable will contain the testing set of the cleaned text content.\n",
    "y_train: This variable will contain the training set of the intensity labels.\n",
    "y_test: This variable will contain the testing set of the intensity labels.\n",
    "\n",
    "Purpose\n",
    "The purpose of this code is to prepare the data for training a machine learning model. By splitting the data into training and testing sets, you can train the model on one subset of data and evaluate its performance on another, unseen subset. This helps in assessing how well the model generalizes to new data.\n",
    "\n",
    "print(X_train.shape[0],X_test.shape[0])\n",
    "checks the shape of train and test data\n",
    "\n",
    "print(y_train.value_counts())\n",
    "checks the class balance in the training data\n",
    "\n",
    "print(y_test.value_counts())\n",
    "checks the target class balance in the test data\n",
    "\n",
    "#### C) Applying TF-IDF Vectorization\n",
    "\n",
    "vectorizertfidf = TfidfVectorizer(use_idf=True, ngram_range=(1, 2))\n",
    "X_train_tfvec = vectorizertfidf.fit_transform(X_train)\n",
    "X_test_tfvec = vectorizertfidf.transform(X_test)\n",
    "\n",
    "TfidfVectorizer(...):\n",
    "\n",
    "This class is part of the sklearn.feature_extraction.text module and is used to convert a collection of raw documents (text data) into a matrix of TF-IDF features.\n",
    "\n",
    "use_idf=True: This parameter indicates that the Inverse Document Frequency (IDF) component of the TF-IDF score should be used in the transformation. IDF helps to weigh down the importance of commonly occurring words and gives more weight to rare words.\n",
    "\n",
    "ngram_range=(1, 2): This parameter specifies the range of n-grams to be extracted. In this case, it will extract both unigrams (1-word sequences) and bigrams (2-word sequences) from the text. This can help capture more context and relationships between words in the documents.\n",
    "\n",
    "fit_transform(X_train):\n",
    "\n",
    "This method fits the vectorizer to the training data X_train and transforms the text data into a TF-IDF feature matrix.\n",
    "The resulting X_train_tfvec will be a sparse matrix where each row corresponds to a document in X_train, and each column corresponds to a feature (word or n-gram) from the text data. The values in the matrix represent the TF-IDF scores of the corresponding words/n-grams in each document.\n",
    "\n",
    "transform(X_test):\n",
    "\n",
    "This method transforms the test data X_test into a TF-IDF feature matrix using the vocabulary learned from the training data (i.e., it applies the same transformation to the test set).\n",
    "The resulting X_test_tfvec will also be a sparse matrix similar to X_train_tfvec, but it will only contain the features that were identified during the fit_transform on the training data. This is important to prevent data leakage, ensuring that the model does not have access to information from the test set during training.\n",
    "\n",
    "Purpose\n",
    "The purpose of this code is to prepare the text data for training a machine learning model by converting the textual information into a numerical format that can be fed into the model. TF-IDF is a popular method for representing text data, as it takes into account both the frequency of words in a document and their importance across the entire corpus.\n",
    "\n",
    "#### D) Training and Evaluation using different Models\n",
    "\n",
    "##### 1) Creating the score-card and defining a function to update it with important perfromance metric\n",
    "\n",
    "score_card = pd.DataFrame(columns=['model_name','Accuracy Score','Precision Score','Recall Score','f1 Score'])\n",
    "\n",
    "def update_score_card(y_test,y_pred,model_name):\n",
    "\n",
    "    # assign 'score_card' as global variable\n",
    "    global score_card\n",
    "\n",
    "    # append the results to the dataframe 'score_card'\n",
    "    # 'ignore_index = True' do not consider the index labels\n",
    "    score_card = pd.concat([score_card,pd.DataFrame([{'model_name':model_name,\n",
    "                                    'Accuracy Score' : accuracy_score(y_test, y_pred),\n",
    "                                    'Precision Score': precision_score(y_test, y_pred, average=\"weighted\"),\n",
    "                                    'Recall Score': recall_score(y_test, y_pred, average=\"weighted\"),\n",
    "                                    'f1 Score': f1_score(y_test, y_pred, average=\"weighted\")}])],\n",
    "                                    ignore_index = True)\n",
    "                                    \n",
    "score_card = pd.DataFrame(columns=['model_name','Accuracy Score','Precision Score','Recall Score','f1 Score'])\n",
    "This line initializes an empty DataFrame named score_card with predefined columns to store the names and performance metrics of different models.\n",
    "\n",
    "def update_score_card(y_test,y_pred,model_name):\n",
    "This function takes three parameters:\n",
    "y_test: The true labels of the test dataset.\n",
    "y_pred: The predicted labels from the model.\n",
    "model_name: A string representing the name of the model being evaluated.\n",
    "\n",
    "global score_card : This line indicates that the score_card DataFrame used in this function is the same as the one defined outside the function. It allows the function to modify the global variable rather than creating a local copy.\n",
    "\n",
    "\n",
    "score_card = pd.concat([score_card, pd.DataFrame([{'model_name': model_name,\n",
    "                                'Accuracy Score': accuracy_score(y_test, y_pred),\n",
    "                                'Precision Score': precision_score(y_test, y_pred, average=\"weighted\"),\n",
    "                                'Recall Score': recall_score(y_test, y_pred, average=\"weighted\"),\n",
    "                                'f1 Score': f1_score(y_test, y_pred, average=\"weighted\")}])],\n",
    "                                ignore_index=True)\n",
    "                                \n",
    "his block performs the following tasks:\n",
    "\n",
    "It creates a new DataFrame containing the performance metrics calculated using the true and predicted labels:\n",
    "\n",
    "Accuracy Score: The proportion of correctly predicted instances out of the total instances.\n",
    "Precision Score: The ratio of true positives to the total predicted positives (weighted by class).\n",
    "Recall Score: The ratio of true positives to the total actual positives (weighted by class).\n",
    "F1 Score: The harmonic mean of precision and recall (weighted by class).\n",
    "\n",
    "It concatenates this new DataFrame with the existing score_card DataFrame and updates the global score_card variable.\n",
    "ignore_index=True: This ensures that the index is reset and does not retain any previous index labels from the original DataFrame.\n",
    "\n",
    "Purpose\n",
    "The purpose of this code is to systematically track the performance of various machine learning models in a structured way. By using the update_score_card function, you can easily evaluate and compare the performance of different models on the same dataset by appending their scores to the scorecard.\n",
    "                                    \n",
    "##### 2) Logistic Regression Model\n",
    "\n",
    "lr = LogisticRegression(class_weight='balanced')\n",
    "Initializes the Logistic Regression Model\n",
    "\n",
    "lr.fit(X_train_tfvec,y_train)\n",
    "Fits the model on the training data\n",
    "\n",
    "y_pred_lr = lr.predict(X_test_tfvec)\n",
    "Obtains preditctions on the test data\n",
    "\n",
    "accuracy_score(y_test,y_pred_lr)\n",
    "Using accuracy_score() we check the accuracy on the testing dataset\n",
    "\n",
    "##### confusion matrix\n",
    "\n",
    "cmlr = confusion_matrix(y_test, y_pred_lr, labels=lr.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmlr, display_labels=lr.classes_)\n",
    "disp.plot()\n",
    "plt.savefig(os.path.join(visuals_folder, \"LR_Confusion_Matrix.png\"))\n",
    "plt.show()\n",
    "\n",
    "cmlr = confusion_matrix(y_test, y_pred_lr, labels=lr.classes_)\n",
    "This computes the confusion matrix for your logistic regression model (lr). It compares the true labels (y_test) with the predicted labels (y_pred_lr).\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmlr, display_labels=lr.classes_)\n",
    "This creates a display object for the confusion matrix (disp), which will be plotted later. The matrix will have labels corresponding to the classes in your model.\n",
    "\n",
    "disp.plot()\n",
    "This generates a plot of the confusion matrix.\n",
    "\n",
    "plt.savefig(os.path.join(visuals_folder, \"LR_Confusion_Matrix.png\"))\n",
    "Saves the confusion matrix plot as an image (LR_Confusion_Matrix.png) in the specified folder (visuals_folder).\n",
    "\n",
    "plt.show()\n",
    "This shows the confusion matrix plot in a window or notebook, depending on your environment.\n",
    "\n",
    "#####\n",
    "\n",
    "print(classification_report(y_pred_lr,y_test))\n",
    "evaluate the performance of logistic regression model on test data using Classification Report\n",
    "\n",
    "update_score_card(y_test,y_pred_lr,'Initial_LR_model')\n",
    "calls the update score card method to update the score card with this model's score\n",
    "\n",
    "##### Performing Hyperparameter tuning\n",
    "\n",
    "param_gridlr = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'newton-cg', 'saga']\n",
    "}\n",
    "defines the hyperparameter grid dictionary\n",
    "\n",
    "grid_searchlr = GridSearchCV(lr, param_gridlr, cv=5, scoring='accuracy')\n",
    "This creates a GridSearchCV object that will:\n",
    "Perform cross-validation with cv=5 (5-fold cross-validation) to evaluate the model’s performance.\n",
    "Optimize accuracy using scoring='accuracy'.\n",
    "Search over the parameter grid param_gridlr to find the best combination of C and solver for the Logistic Regression model (lr).\n",
    "\n",
    "grid_searchlr.fit(X_train_tfvec, y_train)\n",
    "Fits the grid on the training data\n",
    "\n",
    "best_lr = grid_searchlr.best_estimator_\n",
    "Obtaining the Best model and stores it in best_lr\n",
    "\n",
    "##### \n",
    "\n",
    "best_lr - checks the best model\n",
    "best_lr.fit(X_train_tfvec, y_train) - Fits the best model obtained on the training data\n",
    "y_pred_blr = best_lr.predict(X_test_tfvec) - Obtains preditctions on the test data\n",
    "\n",
    "accuracy_score(y_test,y_pred_blr)\n",
    "Using accuracy_score() we check the accuracy on the testing dataset\n",
    "\n",
    "##### Confusion Matrix\n",
    "\n",
    "cmblr = confusion_matrix(y_test, y_pred_blr, labels=best_lr.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmblr, display_labels=best_lr.classes_)\n",
    "disp.plot()\n",
    "plt.savefig(os.path.join(visuals_folder, \"Tuned_LR_Confusion_Matrix.png\"))\n",
    "plt.show()\n",
    "\n",
    "This code snippet plots a confusion matrix for the tuned LR Model. The code does the same thing as the previous confusion matrix code except that it \n",
    "Saves the confusion matrix plot as an image (Tuned_LR_Confusion_Matrix.png) in the specified folder (visuals_folder).\n",
    "\n",
    "#####\n",
    "\n",
    "print(classification_report(y_pred_blr,y_test))\n",
    "evaluates the performance of the tuned logistic regression model on test data using Classification Report\n",
    "\n",
    "update_score_card(y_test,y_pred_blr,'BestTuned_LR_model')\n",
    "calls the update score card method to update the score card with this model's score\n",
    "\n",
    "##### 3) Random Forest Classifier\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "Initializes the Random Forest Classifier Model\n",
    "\n",
    "rf.fit(X_train_tfvec,y_train)\n",
    "Fits it on the training data\n",
    "\n",
    "y_pred_rf = rf.predict(X_test_tfvec)\n",
    "Obtains predictions on the test data\n",
    "\n",
    "accuracy_score(y_test,y_pred_rf) - Using accuracy_score() we check the accuracy on the testing dataset\n",
    "\n",
    "##### Confusion Matrix\n",
    "\n",
    "cmrf = confusion_matrix(y_test, y_pred_rf, labels=rf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmrf, display_labels=rf.classes_)\n",
    "disp.plot()\n",
    "plt.savefig(os.path.join(visuals_folder, \"RFC_Confusion_Matrix.png\"))\n",
    "plt.show()\n",
    "\n",
    "This code snippet plots a confusion matrix for the RFC Model. The code does the same thing as the previous confusion matrix code except that it \n",
    "Saves the confusion matrix plot as an image (RFC_Confusion_Matrix.png) in the specified folder (visuals_folder).\n",
    "\n",
    "##### \n",
    "\n",
    "print(classification_report(y_pred_rf,y_test))\n",
    "evaluates the performance of the random forest classifier model on test data using classification report\n",
    "\n",
    "update_score_card(y_test,y_pred_rf,'Initial_RFC_model')\n",
    "calling the update score card method to update the score card with this model's score\n",
    "\n",
    "##### Performing Hyperparamter tuning\n",
    "\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [10, 50, 100, None]\n",
    "}\n",
    "defines the hyperparameter grid dictionary\n",
    "\n",
    "grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, scoring='accuracy')\n",
    "\n",
    "This creates a GridSearchCV object that will:\n",
    "Perform 5-fold cross-validation (cv=5) to evaluate the Random Forest model's performance.\n",
    "Optimize based on accuracy using scoring='accuracy'.\n",
    "Search through the hyperparameter grid (param_grid_rf) to find the best combination of parameters for the Random Forest model (rf).\n",
    "\n",
    "\n",
    "grid_search_rf.fit(X_train_tfvec, y_train)\n",
    "Fits the grid on the training data\n",
    "\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "print(f\"Best parameters: {grid_search_rf.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search_rf.best_score_}\")\n",
    "\n",
    "This snippet gets and prints the best parameters and model\n",
    "\n",
    "best_rf - checks the best model\n",
    "\n",
    "best_rf.fit(X_train_tfvec,y_train) - Fits the best model obtained on the training data\n",
    "\n",
    "y_pred_brf = rf.predict(X_test_tfvec) - Obtains preditctions on the test data\n",
    "\n",
    "accuracy_score(y_test,y_pred_brf) - Using accuracy_score() we check the accuracy on the testing dataset\n",
    "\n",
    "##### Confusion Matrix\n",
    "\n",
    "cmbrf = confusion_matrix(y_test, y_pred_brf, labels=best_rf.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmbrf, display_labels=best_rf.classes_)\n",
    "disp.plot()\n",
    "plt.savefig(os.path.join(visuals_folder, \"Tuned_RFC_Confusion_Matrix.png\"))\n",
    "plt.show()\n",
    "\n",
    "This code snippet plots a confusion matrix for the Tuned RFC Model. The code does the same thing as the previous confusion matrix code except that it \n",
    "Saves the confusion matrix plot as an image (Tuned_RFC_Confusion_Matrix.png) in the specified folder (visuals_folder).\n",
    "\n",
    "#####\n",
    "\n",
    "print(classification_report(y_pred_brf,y_test)) - evaluates the performance of the Random Forest Classifier model on test data using Classification Report\n",
    "\n",
    "update_score_card(y_test,y_pred_brf,'BestTuned_RFC_model') - calls the update score card method to update the score card with this model's score\n",
    "\n",
    "##### 4) Support Vector Machine Classifier\n",
    "\n",
    "\n",
    "SVC = LinearSVC()\n",
    "Initializes the Support Vector Machine Classifier\n",
    "\n",
    "SVC.fit(X_train_tfvec,y_train)\n",
    "Fits the model on the training data\n",
    "\n",
    "y_pred_svc = SVC.predict(X_test_tfvec)\n",
    "Obtains the predictions on the test data\n",
    "\n",
    "accuracy_score(y_test,y_pred_svc) - Using accuracy_score() we check the accuracy on the testing dataset\n",
    "\n",
    "##### Confusion Matrix\n",
    "\n",
    "cmsvc = confusion_matrix(y_test, y_pred_svc, labels=SVC.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmsvc, display_labels=SVC.classes_)\n",
    "disp.plot()\n",
    "plt.savefig(os.path.join(visuals_folder, \"SVC_Confusion_Matrix.png\"))\n",
    "plt.show()\n",
    "\n",
    "This code snippet plots a confusion matrix for the SVC Model. The code does the same thing as the previous confusion matrix code except that it \n",
    "Saves the confusion matrix plot as an image (SVC_Confusion_Matrix.png) in the specified folder (visuals_folder).\n",
    "\n",
    "#####\n",
    "\n",
    "print(classification_report(y_pred_svc,y_test))\n",
    "evaluates the performance of the Support Vector Classifier on test data using Classification Report\n",
    "\n",
    "update_score_card(y_test,y_pred_svc,'Initial_SVC_model')\n",
    "calls the update score card method to update the score card with this model's score\n",
    "\n",
    "##### Performing Hyperparameter tuning\n",
    "\n",
    "\n",
    "param_grid_svc = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "defines the hyperparameter grid dictionary\n",
    "\n",
    "grid_search_svc = GridSearchCV(SVC, param_grid_svc, cv=5, scoring='accuracy')\n",
    "\n",
    "This creates a GridSearchCV object that will:\n",
    "Perform 5-fold cross-validation (cv=5) to evaluate the Support Vector Classifier (SVC) model's performance.\n",
    "Optimize based on accuracy using scoring='accuracy'.\n",
    "Search through the hyperparameter grid (param_grid_svc) to find the best combination of parameters for the SVC model.\n",
    "\n",
    "grid_search_svc.fit(X_train_tfvec, y_train)\n",
    "Fits the grid on the training data\n",
    "\n",
    "best_svc = grid_search_svc.best_estimator_\n",
    "print(f\"Best parameters: {grid_search_svc.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search_svc.best_score_}\")\n",
    "\n",
    "This snippet Gets and prints the best model and parameters\n",
    "\n",
    "best_svc - checks the best model\n",
    "best_svc.fit(X_train_tfvec, y_train) - Fits the best model obtained on the training data\n",
    "\n",
    "y_pred_bsvc = best_svc.predict(X_test_tfvec) - Obtains the predictions on the test data\n",
    "\n",
    "accuracy_score(y_test,y_pred_bsvc) - Using accuracy_score() we check the accuracy on the testing dataset\n",
    "\n",
    "##### Confusion Matrix\n",
    "\n",
    "cmbsvc = confusion_matrix(y_test, y_pred_bsvc, labels=best_svc.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmbsvc, display_labels=best_svc.classes_)\n",
    "disp.plot()\n",
    "plt.savefig(os.path.join(visuals_folder, \"Tuned_SVC_Confusion_Matrix.png\"))\n",
    "plt.show()\n",
    "\n",
    "This code snippet plots a confusion matrix for the Tuned SVC Model. The code does the same thing as the previous confusion matrix code except that it saves the confusion matrix plot as an image (Tuned_SVC_Confusion_Matrix.png) in the specified folder (visuals_folder).\n",
    "\n",
    "#####\n",
    "\n",
    "print(classification_report(y_pred_bsvc,y_test))\n",
    "evaluate the performance of the tuned Support Vector Classifier model on test data using Classification Report\n",
    "\n",
    "update_score_card(y_test,y_pred_bsvc,'BestTuned_SVC_model')\n",
    "calls the update score card method to update the score card with this model's score\n",
    "\n",
    "#### E) Data Augmentation\n",
    "\n",
    "\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "Appends the original datasets once-again (concatenate them row-wise) for data augmentation\n",
    "\n",
    "import random\n",
    "from nltk.corpus import wordnet  - Imports addidtional libraries\n",
    "\n",
    "nltk.download('wordnet') - Downloads the wordnet lexical database\n",
    "\n",
    "##### Defining various Functions for augmentation\n",
    "\n",
    "##### 1) The function get_synonyms(word) retrieves synonyms for a given word using the WordNet lexical database or \n",
    "#####  returns an empty list if no synonyms are found.\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = wordnet.synsets(word) # This line fetches all the synsets (sets of synonyms) associated with the input word.\n",
    "    if not synonyms:                 # It checks if there are no synsets found. If none are found, it returns an empty list.\n",
    "        return []\n",
    "    return list(set([lemma.name() for synonym in synonyms for lemma in synonym.lemmas()])) \n",
    "    \n",
    "The function get_synonyms(word) retrieves synonyms for a given word using the WordNet lexical database. Here’s a brief breakdown of its functionality:\n",
    "\n",
    "wordnet.synsets(word): This line fetches all the synsets (sets of synonyms) associated with the input word.\n",
    "\n",
    "if not synonyms:: It checks if there are no synsets found. If none are found, it returns an empty list.\n",
    "\n",
    "return list(set([...])): This line constructs a list of unique synonyms:\n",
    "\n",
    "It iterates over each synonym in the synonyms list.\n",
    "For each synonym, it retrieves the associated lemmas (word forms).\n",
    "Using set() ensures that the synonyms are unique, and the result is converted back to a list.\n",
    "In Summary:\n",
    "The function returns a list of unique synonyms for a specified word using WordNet, or an empty list if no synonyms are found.\n",
    "\n",
    "##### 2) The function synonym_replacement(sentence, n) performs synonym replacement in a given sentence. \n",
    "##### The function replaces up to n words in a sentence with their randomly selected synonyms, enhancing the variety of the sentence \n",
    "##### while retaining its overall meaning.\n",
    "\n",
    "def synonym_replacement(sentence, n):\n",
    "    words = sentence.split()          # The sentence is split into a list of individual words.\n",
    "    new_words = words.copy()          # A copy of the original list of words is created to store the modified sentence.\n",
    "    \n",
    "    random_word_list = list(set([word for word in words if get_synonyms(word)])) #This list contains unique words from the \n",
    "    # original sentence that have synonyms available (obtained from the get_synonyms function).\n",
    "    \n",
    "    random.shuffle(random_word_list) # The random_word_list is shuffled to randomize the order of words for replacement.\n",
    "\n",
    "    num_replaced = 0\n",
    "    for random_word in random_word_list:   # The function iterates over the shuffled random_word_list, \n",
    "                                           # retrieving synonyms for each word:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        if synonyms:             # If synonyms exist, it randomly selects one and replaces the corresponding word in new_words.\n",
    "            synonym = random.choice(synonyms)\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1    # The count of replaced words (num_replaced) is incremented.\n",
    "        if num_replaced >= n:    # The loop stops when num_replaced reaches the specified limit n.\n",
    "            break\n",
    "             \n",
    "    return ' '.join(new_words) # The modified list of words is joined back into a single string and returned as the output.\n",
    "\n",
    "The function synonym_replacement(sentence, n) performs synonym replacement in a given sentence. Here’s a brief breakdown of its functionality:\n",
    "\n",
    "Input Parameters:\n",
    "\n",
    "sentence: A string input that represents the sentence in which synonyms will be replaced.\n",
    "n: An integer specifying the maximum number of words to replace with their synonyms.\n",
    "\n",
    "Word Splitting:\n",
    "\n",
    "words = sentence.split(): The sentence is split into a list of individual words.\n",
    "Copying Words:\n",
    "\n",
    "new_words = words.copy(): A copy of the original list of words is created to store the modified sentence.\n",
    "\n",
    "Random Word List:\n",
    "\n",
    "random_word_list: This list contains unique words from the original sentence that have synonyms available (obtained from the get_synonyms function).\n",
    "\n",
    "Shuffling:\n",
    "\n",
    "random.shuffle(random_word_list): The random_word_list is shuffled to randomize the order of words for replacement.\n",
    "\n",
    "Synonym Replacement:\n",
    "\n",
    "The function iterates over the shuffled random_word_list, retrieving synonyms for each word:\n",
    "If synonyms exist, it randomly selects one and replaces the corresponding word in new_words.\n",
    "The count of replaced words (num_replaced) is incremented.\n",
    "The loop stops when num_replaced reaches the specified limit n.\n",
    "\n",
    "Output:\n",
    "\n",
    "return ' '.join(new_words): The modified list of words is joined back into a single string and returned as the output.\n",
    "\n",
    "In Summary:\n",
    "\n",
    "The function replaces up to n words in a sentence with their randomly selected synonyms, enhancing the variety of the sentence while retaining its overall meaning.\n",
    "\n",
    "##### 3) The function random_insertion(sentence, n) adds random synonyms into a given sentence.\n",
    "##### The function randomly inserts up to n synonyms from the original sentence into various positions, \n",
    "##### enhancing the sentence's richness while retaining its basic structure.\n",
    "\n",
    "def random_insertion(sentence, n):\n",
    "    words = sentence.split()    # The sentence is split into a list of individual words.\n",
    "    new_words = words.copy()    # A copy of the original list of words is created to store the modified sentence.\n",
    "    for _ in range(n):          # The loop iterates n times to insert synonyms\n",
    "        if len(words) == 0:  # Checks if there are any words left to choose from. If not, it breaks out of the loop.\n",
    "            break\n",
    "\n",
    "        synonym = get_synonyms(random.choice(words)) # A random word from the original list is selected, and its synonyms are retrieved.\n",
    "        if synonym:                             # If synonyms are available, a random position is selected for insertion.\n",
    "            insert_pos = random.randint(0, len(new_words))  # Allow inserting at the end\n",
    "            new_words.insert(insert_pos, random.choice(synonym)) # A randomly chosen synonym is inserted into the \n",
    "                                                                 # new_words list at the selected position.\n",
    "    return ' '.join(new_words) # The modified list of words is joined back into a single string and returned as the output.\n",
    "\n",
    "\n",
    "The function random_insertion(sentence, n) adds random synonyms into a given sentence. Here’s a brief breakdown of its functionality:\n",
    "\n",
    "Input Parameters:\n",
    "\n",
    "sentence: A string representing the original sentence.\n",
    "n: An integer specifying the number of synonyms to insert into the sentence.\n",
    "\n",
    "Word Splitting:\n",
    "\n",
    "words = sentence.split(): The sentence is split into a list of individual words.\n",
    "\n",
    "Copying Words:\n",
    "\n",
    "new_words = words.copy(): A copy of the original list of words is created to store the modified sentence.\n",
    "\n",
    "Random Insertion Loop:\n",
    "\n",
    "The loop iterates n times to insert synonyms:\n",
    "if len(words) == 0:: Checks if there are any words left to choose from. If not, it breaks out of the loop.\n",
    "synonym = get_synonyms(random.choice(words)): A random word from the original list is selected, and its synonyms are retrieved.\n",
    "\n",
    "if synonym:: If synonyms are available, a random position is selected for insertion.\n",
    "new_words.insert(insert_pos, random.choice(synonym)): A randomly chosen synonym is inserted into the new_words list at the selected position.\n",
    "\n",
    "Output:\n",
    "\n",
    "return ' '.join(new_words): The modified list of words is joined back into a single string and returned as the output.\n",
    "\n",
    "In Summary:\n",
    "The function randomly inserts up to n synonyms from the original sentence into various positions, enhancing the sentence's richness while retaining its basic structure.\n",
    "\n",
    "###### 4) The function random_swap(sentence, n) randomly swaps pairs of words in a given sentence.\n",
    "\n",
    "def random_swap(sentence, n):\n",
    "    words = sentence.split()   # The sentence is split into a list of individual words.\n",
    "    new_words = words.copy()   # A copy of the original list of words is created to store the modified sentence.\n",
    "    if len(words) < 2:  # Ensure there are at least 2 words to swap\n",
    "        return sentence\n",
    "\n",
    "    for _ in range(n):  # The loop iterates n times to perform word swaps\n",
    "        idx1, idx2 = random.sample(range(len(words)), 2)  # Randomly selects two distinct indices from the list of words.\n",
    "        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1] # The words at the two selected indices are swapped \n",
    "                                                                            # in the new_words list.\n",
    "    return ' '.join(new_words) # The modified list of words is joined back into a single string and returned as the output.\n",
    "\n",
    "##### The function randomly swaps up to n pairs of words in a sentence, altering the sentence's structure while retaining the \n",
    "##### original words. If there are fewer than two words, it simply returns the original sentence.\n",
    "\n",
    "The function random_swap(sentence, n) randomly swaps pairs of words in a given sentence. Here’s a brief breakdown of its functionality:\n",
    "\n",
    "Input Parameters:\n",
    "\n",
    "sentence: A string representing the original sentence.\n",
    "n: An integer specifying the number of pairs of words to swap.\n",
    "Word Splitting:\n",
    "\n",
    "words = sentence.split(): The sentence is split into a list of individual words.\n",
    "Copying Words:\n",
    "\n",
    "new_words = words.copy(): A copy of the original list of words is created to store the modified sentence.\n",
    "Check for Minimum Words:\n",
    "\n",
    "if len(words) < 2:: This condition checks if there are fewer than two words in the sentence. If true, it returns the original sentence since there are not enough words to swap.\n",
    "\n",
    "Random Swapping Loop:\n",
    "\n",
    "The loop iterates n times to perform word swaps:\n",
    "idx1, idx2 = random.sample(range(len(words)), 2): Randomly selects two distinct indices from the list of words.\n",
    "new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]: The words at the two selected indices are swapped in the new_words list.\n",
    "\n",
    "Output:\n",
    "\n",
    "return ' '.join(new_words): The modified list of words is joined back into a single string and returned as the output.\n",
    "\n",
    "In Summary:\n",
    "The function randomly swaps up to n pairs of words in a sentence, altering the sentence's structure while retaining the original words. If there are fewer than two words, it simply returns the original sentence.\n",
    "\n",
    "##### 5) The function random_deletion(sentence, p=0.3) randomly deletes words from a given sentence based on a specified probability. \n",
    "\n",
    "def random_deletion(sentence, p=0.3): # p is A float representing the probability of deleting each word\n",
    "                                      # (default is 0.3, meaning there is a 30% chance for each word to be deleted).\n",
    "    words = sentence.split()          # The sentence is split into a list of individual words.\n",
    "    if len(words) <= 1:  # If there's only one word, don't delete it\n",
    "        return sentence\n",
    "\n",
    "    # Remove words with probability p\n",
    "    new_words = [word for word in words if random.uniform(0, 1) > p] \n",
    "    # This line creates a new list of words (new_words) that includes words from the original list only if a random number \n",
    "    # (between 0 and 1) is greater than the probability p. This effectively removes words with a 30% chance of being deleted.\n",
    "\n",
    "    # Check if new_words is empty; if so, return the original sentence or the first word\n",
    "    if not new_words:\n",
    "        return words[0]  # Return the first word to avoid empty sentence\n",
    "\n",
    "    return ' '.join(new_words) # The modified list of words is joined back into a single string and returned as the output.\n",
    "\n",
    "##### The function randomly deletes words from a sentence based on a specified probability. If the sentence has only one word, \n",
    "##### it returns the original sentence. If all words are deleted, it returns the first word to prevent an empty output.\n",
    "\n",
    "The function random_deletion(sentence, p=0.3) randomly deletes words from a given sentence based on a specified probability. Here’s a brief breakdown of its functionality:\n",
    "\n",
    "Input Parameters:\n",
    "\n",
    "sentence: A string representing the original sentence.\n",
    "p: A float representing the probability of deleting each word (default is 0.3, meaning there is a 30% chance for each word to be deleted).\n",
    "\n",
    "Word Splitting:\n",
    "\n",
    "words = sentence.split(): The sentence is split into a list of individual words.\n",
    "\n",
    "Check for Minimum Words:\n",
    "\n",
    "if len(words) <= 1:: If there is only one word in the sentence, it simply returns the original sentence without any deletion.\n",
    "\n",
    "Random Deletion:\n",
    "\n",
    "new_words = [word for word in words if random.uniform(0, 1) > p]: This line creates a new list of words (new_words) that includes words from the original list only if a random number (between 0 and 1) is greater than the probability p. This effectively removes words with a 30% chance of being deleted.\n",
    "\n",
    "Check for Empty Result:\n",
    "\n",
    "if not new_words:: If no words remain after deletion, it returns the first word of the original list to avoid returning an empty sentence.\n",
    "\n",
    "Output:\n",
    "\n",
    "return ' '.join(new_words): The modified list of words is joined back into a single string and returned as the output.\n",
    "\n",
    "In Summary:\n",
    "The function randomly deletes words from a sentence based on a specified probability. If the sentence has only one word, it returns the original sentence. If all words are deleted, it returns the first word to prevent an empty output.\n",
    "\n",
    "\n",
    "##### 6) The function eda(sentence, num_aug=4) performs various data augmentation techniques on a given sentence to \n",
    "#####   generate multiple augmented versions of it\n",
    "\n",
    "def eda(sentence, num_aug=4): # num_aug is an integer indicating the number of augmented sentences to create (default is 4).\n",
    "    augmented_sentences = []  # This initializes an empty list to store the augmented sentences.\n",
    "    augmented_sentences.append(synonym_replacement(sentence, n=2)) # Replaces two words in the sentence with their synonyms.\n",
    "    augmented_sentences.append(random_insertion(sentence, n=2))    # Inserts two random synonyms from the sentence into various positions.\n",
    "    augmented_sentences.append(random_swap(sentence, n=2))         # Swaps two random words in the sentence.\n",
    "    augmented_sentences.append(random_deletion(sentence, p=0.3))   # Deletes words from the sentence with a probability of 30%.\n",
    "    return augmented_sentences\n",
    "\n",
    "##### The function generates a list of augmented versions of a given sentence using various techniques \n",
    "##### (synonym replacement, random insertion, random swapping, and random deletion), \n",
    "##### enhancing the data for tasks such as natural language processing or machine learning.\n",
    "\n",
    "The function eda(sentence, num_aug=4) performs various data augmentation techniques on a given sentence to generate multiple augmented versions of it. Here’s a brief breakdown of its functionality:\n",
    "\n",
    "Input Parameters:\n",
    "\n",
    "sentence: A string representing the original sentence to be augmented.\n",
    "num_aug: An integer indicating the number of augmented sentences to create (default is 4).\n",
    "\n",
    "List Initialization:\n",
    "\n",
    "augmented_sentences = []: This initializes an empty list to store the augmented sentences.\n",
    "\n",
    "Augmentation Techniques:\n",
    "\n",
    "The function applies four different augmentation techniques and appends the results to the augmented_sentences list:\n",
    "\n",
    "synonym_replacement(sentence, n=2): Replaces two words in the sentence with their synonyms.\n",
    "random_insertion(sentence, n=2): Inserts two random synonyms from the sentence into various positions.\n",
    "random_swap(sentence, n=2): Swaps two random words in the sentence.\n",
    "random_deletion(sentence, p=0.3): Deletes words from the sentence with a probability of 30%.\n",
    "\n",
    "Output:\n",
    "\n",
    "return augmented_sentences: The function returns the list of augmented sentences.\n",
    "\n",
    "In Summary:\n",
    "The function generates a list of augmented versions of a given sentence using various techniques (synonym replacement, random insertion, random swapping, and random deletion), enhancing the data for tasks such as natural language processing or machine learning.\n",
    "\n",
    "##### 7) This code augments a dataset by generating new sentences for each entry using different augmentation techniques. \n",
    "##### It creates a new DataFrame to hold the original and augmented sentences, \n",
    "##### which can be combined with the original data for further analysis or modeling.\n",
    "\n",
    "##### Augment the dataset\n",
    "augmented_sentences = [] # Initializes an empty list to store augmented sentences.\n",
    "augmented_labels = []    # Initializes an empty list to store labels corresponding to the augmented sentences.\n",
    "\n",
    "\n",
    "for index, row in df.iterrows(): # Iterates through each row in the DataFrame df\n",
    "    sentence = row['content']    # Retrieves the content (sentence) from the current row.\n",
    "    label = row['intensity']     # Retrieves the intensity label associated with the current sentence.\n",
    "\n",
    "    # Generate augmented sentences for each row\n",
    "    aug_sentences = eda(sentence, num_aug=4) # Calls the eda function to generate four augmented sentences \n",
    "                                             # for the current sentence.\n",
    "\n",
    "    # Append the original and augmented sentences along with labels\n",
    "    augmented_sentences.append(sentence)  # Adds the original sentence to the augmented_sentences list.\n",
    "    augmented_labels.append(label)  # Adds the corresponding label for the original sentence to the augmented_labels list.\n",
    "\n",
    "    # Add augmented data\n",
    "    augmented_sentences.extend(aug_sentences) # Adds the newly generated augmented sentences to the list.\n",
    "    augmented_labels.extend([label] * len(aug_sentences)) # Duplicates the original label for each of the augmented sentences \n",
    "                                                          # and appends it to the augmented_labels list.\n",
    "\n",
    "##### Constructs a new DataFrame containing the augmented sentences and their corresponding labels.\n",
    "augmented_df = pd.DataFrame({\n",
    "    'content': augmented_sentences,  \n",
    "    'intensity': augmented_labels\n",
    "})\n",
    "\n",
    "\n",
    "##### Combine original and augmented data (optional, if you want to include the original data too)\n",
    "final_df = pd.concat([df, augmented_df])\n",
    "\n",
    "##### Prints the final DataFrame containing both original and augmented data.\n",
    "print(final_df)\n",
    "\n",
    "In Summary:\n",
    "This code augments a dataset by generating new sentences for each entry using different augmentation techniques. It creates a new DataFrame to hold the original and augmented sentences, which can be combined with the original data for further analysis or modeling.\n",
    "\n",
    "#####\n",
    "\n",
    "augmented_df - checks the augmented_df dataframe\n",
    "\n",
    "file_path = os.path.join(\"C:/Users/nikde/Documents/UpGrad/intensityanalysis/data/processeddata\", \"augmented_data.csv\")\n",
    "Specifies the file path where you want to save the file\n",
    "\n",
    "augmented_df.to_csv(file_path, index=False)  \n",
    "Saves the df dataframe as a CSV file\n",
    "Set index=False to avoid saving the DataFrame index\n",
    "\n",
    "final_df - checks final_df\n",
    "\n",
    "final_df.shape - checks the shape of final_df\n",
    "\n",
    "file_path = os.path.join(\"C:/Users/nikde/Documents/UpGrad/intensityanalysis/data/processeddata\", \"original_appended+augmented_data.csv\")\n",
    "Specifies the file path where you want to save the file\n",
    "\n",
    "final_df.to_csv(file_path, index=False)  \n",
    "Saves the df dataframe as a CSV file\n",
    "Sets index=False to avoid saving the DataFrame index\n",
    "\n",
    "#### F) Performing Exploratory Data Analysis and Feature Engineering after Augmentation¶\n",
    "\n",
    "final_df.info() - This provides a concise summary of the final_df DataFrame, including the number of entries, column names, data types, and non-null counts.\n",
    "\n",
    "final_df.isna().sum()/len(df) * 100 \n",
    "This line calculates the percentage of missing (NaN) values in each column of the DataFrame df.\n",
    "\n",
    "final_df.isna().sum() - counts the number of missing values per column.\n",
    "\n",
    "Dividing by len(final_df) (the total number of rows) and multiplying by 100 gives the percentage of missing values for each column.\n",
    "\n",
    "final_df.duplicated().sum() - checks the total duplicates in final_df\n",
    "\n",
    "visuals_folder_aa = \"C:/Users/nikde/Documents/UpGrad/intensityanalysis/visuals/afteraugmentation\" \n",
    "Defines a folder to save augmented visualizations\n",
    "\n",
    "##### Plotting the target class distribution:\n",
    "\n",
    "label_dist = final_df['intensity'].value_counts().to_dict() \n",
    "fig = plt.figure(figsize = (10, 5)) \n",
    "ax = plt.bar(label_dist.keys(), label_dist.values(), width=0.25) \n",
    "plt.xticks([0,1,2]) \n",
    "plt.xlabel(\"Sentiment\") \n",
    "plt.ylabel(\"Content Count\") \n",
    "plt.savefig(os.path.join(visuals_folder, \"initial_sentiment_distribution.png\")) \n",
    "plt.show()\n",
    "\n",
    "final_df['intensity'].value_counts(): Counts the occurrences of each unique value in the 'intensity' column, which is assumed to be the target or sentiment label (e.g., 0, 1, 2 representing different sentiment categories).\n",
    "\n",
    ".to_dict(): Converts the resulting counts into a dictionary where the keys are the unique intensity labels, and the values are their corresponding counts.\n",
    "\n",
    "plt.figure(figsize=(10, 5)): Creates a new figure for the plot with a specified width (10) and height (5).\n",
    "\n",
    "plt.bar(): Generates a bar plot where the x-axis is the intensity labels (keys of the label_dist dictionary), and the y-axis is their respective counts (values of the dictionary).\n",
    "\n",
    "width=0.25: Sets the width of each bar in the plot.\n",
    "\n",
    "plt.xticks([0,1,2]): Specifies the tick labels on the x-axis, which correspond to the sentiment categories (0, 1, 2).\n",
    "\n",
    "plt.xlabel(\"Sentiment\"): Labels the x-axis as \"Sentiment\".\n",
    "\n",
    "plt.ylabel(\"Content Count\"): Labels the y-axis as \"Content Count\".\n",
    "\n",
    "plt.savefig(): Saves the generated plot as a PNG image to the specified file path (visuals_folder_aa), with the file name sentiment_distribution_after_augmentation.png.\n",
    "\n",
    "plt.show(): Displays the plot on the screen.\n",
    "\n",
    "#####\n",
    "\n",
    "final_df['length']=final_df['content'].apply(lambda x: len(x.split(' '))) \n",
    "adds a new column for the length of the reveiws stored in df['content']\n",
    "\n",
    "final_df.head(10) - Shows the first 10 rows of final_df\n",
    "\n",
    "print(round(final_df[final_df['intensity']=='angriness']['length'].mean()))\n",
    "print(round(final_df[final_df['intensity']=='happiness']['length'].mean()))\n",
    "print(round(final_df[final_df['intensity']=='sadness']['length'].mean()))\n",
    "\n",
    "prints the mean length for angriness, happiness and sadness reviews\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "stopwords.words('english'): This function call comes from the nltk library (Natural Language Toolkit). It retrieves a list of English stopwords, which are words that typically carry little meaning and are usually removed in text preprocessing. Common examples of stopwords include words like \"the,\" \"is,\" \"in,\" \"and,\" etc.\n",
    "\n",
    "set(...): The set function converts the list of stopwords into a set. Using a set is beneficial because:\n",
    "\n",
    "Faster Lookup: Sets provide faster membership testing compared to lists. This means checking if a word is a stopword will be quicker.\n",
    "\n",
    "No Duplicates: A set automatically eliminates any duplicate entries, ensuring that each stopword is only represented once.\n",
    "\n",
    "Purpose:\n",
    "\n",
    "The purpose of creating the STOPWORDS set is to prepare for text preprocessing tasks, such as tokenization or text classification, where you may want to exclude these common words to focus on more meaningful words in the text.\n",
    "\n",
    "final_df[\"clean_content\"]=final_df[\"content\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (STOPWORDS)]))\n",
    "\n",
    "final_df[\"content\"]: This references the \"content\" column in the DataFrame final_df, which presumably contains raw text data (e.g., user reviews, comments, etc.).\n",
    "\n",
    ".apply(...): The apply function is used to apply a function to each element of the Series (in this case, each row in the \"content\" column).\n",
    "\n",
    "lambda x: ...: This defines an anonymous (lambda) function that takes an input x, representing a single piece of text from the \"content\" column.\n",
    "\n",
    "x.split(): This method splits the string x into a list of words (tokens) based on whitespace. For example, the sentence \"This is a test.\" would become [\"This\", \"is\", \"a\", \"test.\"].\n",
    "\n",
    "[word for word in x.split() if word not in STOPWORDS]: This list comprehension iterates over each word in the list created by x.split(). It constructs a new list that includes only the words that are not in the STOPWORDS set. Essentially, it filters out any stopwords.\n",
    "\n",
    "' '.join(...): After filtering, the join method concatenates the remaining words back into a single string, with a space ' ' between each word.\n",
    "\n",
    "final_df[\"clean_content\"] = ...: The resulting cleaned text (with stopwords removed) is assigned to a new column named \"clean_content\" in the DataFrame final_df.\n",
    "\n",
    "Purpose: The purpose of this code is to preprocess the text data in the \"content\" column by removing common, less meaningful words (stopwords). The cleaned text is stored in a new column (\"clean_content\"), which can be used for further analysis or modeling tasks, such as sentiment analysis or text classification.\n",
    "\n",
    "final_df.head(10) - Shows the first 10 rows of final_df again\n",
    "\n",
    "final_df['clean_content'] = final_df['clean_content'].apply(text_clean) applies the function on the clean_content column\n",
    "\n",
    "final_df.head(10) - Shows the first 10 rows of final_df after cleaning\n",
    "\n",
    "final_df = final_df.drop('length', axis = 1) - drops the length column\n",
    "\n",
    "final_df.head() - checks final_df for the change\n",
    "\n",
    "final_df['length']=final_df['clean_content'].apply(lambda x: len(x.split(' '))) adds back the length column after the clean_content column for the length of the cleaned reveiws\n",
    "\n",
    "final_df.head(5) - checks final_df for the change\n",
    "\n",
    "final_df[\"clean_content\"]=final_df[\"clean_content\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (STOPWORDS)]))\n",
    "\n",
    "removes the stop words again because of new words created after applying the text_clean function\n",
    "\n",
    "final_df['length']=final_df['clean_content'].apply(lambda x: len(x.split(' '))) updates the length column after removing stopwords again for the length of the cleaned reveiws\n",
    "\n",
    "final_df.head() - inspects the first 5 rows of final_df lastly\n",
    "\n",
    "print(round(final_df[final_df['intensity']=='angriness']['length'].mean()))\n",
    "print(round(final_df[final_df['intensity']=='happiness']['length'].mean()))\n",
    "print(round(final_df[final_df['intensity']=='sadness']['length'].mean()))\n",
    "\n",
    "prints the mean length for angriness, happiness and sadness reviews\n",
    "\n",
    "##### Plotting the distribution based on the length of the reveiews for each intensity after cleaning\n",
    "\n",
    "The explanation is same as in the non-augmented plot except\n",
    "\n",
    "plt.savefig(os.path.join(visuals_folder, \"reveiw_len_dist_after_augmentation.png\")) Saves the histogram plot as a PNG image in the specified directory (visuals_folder_aa) with the filename reveiw_len_dist_after_augmentation.png\n",
    "\n",
    "final_df.head(10) - inspects the processed dataset final_df\n",
    "\n",
    "file_path = os.path.join(\"C:/Users/nikde/Documents/UpGrad/intensityanalysis/data/processeddata\", \"final_processed_data.csv\")\n",
    "Specifies the file path where you want to save the file\n",
    "\n",
    "final_df.to_csv(file_path, index=False)\n",
    "Saves the final_df dataframe as a CSV file\n",
    "Set index=False to avoid saving the DataFrame index\n",
    "\n",
    "##### Displaying the top 50 words by frequency\n",
    "\n",
    "top_words = 50 Setting the number of top words: This variable defines how many of the most common words you want to extract from the dataset.\n",
    "\n",
    "words = nltk.tokenize.word_tokenize(final_df['clean_content'].str.cat(sep=' '))\n",
    "\n",
    "Tokenizing the cleaned content: final_df['clean_content'].str.cat(sep=' '): This part concatenates all the entries in the clean_content column of the DataFrame final_df into a single string, separating each entry with a space.\n",
    "\n",
    "nltk.tokenize.word_tokenize(...): The word_tokenize function from the Natural Language Toolkit (NLTK) is then used to split this concatenated string into individual words (tokens). This results in a list of words from all the cleaned text in the DataFrame.\n",
    "\n",
    "filter_words = [word for word in words if word not in STOPWORDS]\n",
    "\n",
    "Filtering out stopwords:\n",
    "\n",
    "This line uses a list comprehension to iterate over the list of words.\n",
    "\n",
    "For each word, it checks whether it is not in the predefined STOPWORDS set (which typically includes common words like \"the\", \"is\", \"in\", etc., that do not carry significant meaning in text analysis).\n",
    "\n",
    "Only words that are not in the stopwords list are included in the filter_words list.\n",
    "\n",
    "word_freq = nltk.FreqDist(filter_words)\n",
    "\n",
    "nltk.FreqDist(...): This function from the NLTK (Natural Language Toolkit) library creates a frequency distribution of the given input. In this case, the input is filter_words, which is a list of words that have already been filtered to exclude common stopwords.\n",
    "\n",
    "filter_words: This list contains significant words from the cleaned text data after removing stopwords.\n",
    "\n",
    "Purpose The purpose of creating a frequency distribution is to count how many times each word appears in the filter_words list. The result, word_freq, will be a dictionary-like object where:\n",
    "\n",
    "The keys are the unique words from filter_words. The values are the counts of how many times each word appears.\n",
    "\n",
    "final_wordfreq_df = pd.DataFrame(word_freq.most_common(top_words), columns=['Word', 'Frequency'])\n",
    "\n",
    "word_freq.most_common(top_words):\n",
    "\n",
    "This method retrieves the top_words most common words from the word_freq frequency distribution. It returns a list of tuples, where each tuple consists of a word and its corresponding frequency count.\n",
    "\n",
    "pd.DataFrame(...):\n",
    "\n",
    "This part of the code uses pandas to create a DataFrame from the list of tuples generated by most_common(). The DataFrame is structured in two columns: one for the words ('Word') and one for their frequencies ('Frequency'). \n",
    "\n",
    "columns=['Word', 'Frequency']:\n",
    "\n",
    "This argument specifies the names of the columns in the resulting DataFrame, making it clear what data each column represents.\n",
    "The purpose of this code is to organize the frequency data into a structured format that can be easily analyzed, manipulated, or visualized. A DataFrame is a convenient way to hold tabular data, and it allows for easy access to specific rows, filtering, sorting, and more.\n",
    "\n",
    "final_wordfreq_df - displays the final_wordfreq_df dataframe\n",
    "\n",
    "file_path = os.path.join(\"C:/Users/nikde/Documents/UpGrad/intensityanalysis/data/processeddata\",\"final_word_frequency_data.csv\")\n",
    "Specifies the file path where you want to save the file\n",
    "\n",
    "final_wordfreq_df.to_csv(file_path, index=False)\n",
    "Save the final_wordfreq_df dataframe as a CSV file Set index=False to avoid saving the DataFrame index\n",
    "\n",
    "#### G) Performing Train Test Split after Augmentation\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_df[\"clean_content\"],\n",
    "                                                    final_df[\"intensity\"],test_size=0.2,\n",
    "                                                    stratify=final_df['intensity'])\n",
    "                                                    \n",
    "splits the data into training and test set with balanced split based on target class\n",
    "\n",
    "print(X_train.shape[0],X_test.shape[0]) checks the shape of train and test data\n",
    "\n",
    "print(y_train.value_counts()) checks the class balance in the training data\n",
    "\n",
    "print(y_test.value_counts()) checks the target class balance in the test data\n",
    "\n",
    "#### H) Applying TF-IDF Vectorization after augmentation\n",
    "\n",
    "vectorizertfidf = TfidfVectorizer(use_idf=True, ngram_range=(1, 2)) X_train_tfvec = vectorizertfidf.fit_transform(X_train) X_test_tfvec = vectorizertfidf.transform(X_test)\n",
    "\n",
    "TfidfVectorizer(...):\n",
    "\n",
    "This class is part of the sklearn.feature_extraction.text module and is used to convert a collection of raw documents (text data) into a matrix of TF-IDF features.\n",
    "\n",
    "use_idf=True: This parameter indicates that the Inverse Document Frequency (IDF) component of the TF-IDF score should be used in the transformation. IDF helps to weigh down the importance of commonly occurring words and gives more weight to rare words.\n",
    "\n",
    "ngram_range=(1, 2): This parameter specifies the range of n-grams to be extracted. In this case, it will extract both unigrams (1-word sequences) and bigrams (2-word sequences) from the text. This can help capture more context and relationships between words in the documents.\n",
    "\n",
    "fit_transform(X_train):\n",
    "\n",
    "This method fits the vectorizer to the training data X_train and transforms the text data into a TF-IDF feature matrix. The resulting X_train_tfvec will be a sparse matrix where each row corresponds to a document in X_train, and each column corresponds to a feature (word or n-gram) from the text data. The values in the matrix represent the TF-IDF scores of the corresponding words/n-grams in each document.\n",
    "\n",
    "transform(X_test):\n",
    "\n",
    "This method transforms the test data X_test into a TF-IDF feature matrix using the vocabulary learned from the training data (i.e., it applies the same transformation to the test set). The resulting X_test_tfvec will also be a sparse matrix similar to X_train_tfvec, but it will only contain the features that were identified during the fit_transform on the training data. This is important to prevent data leakage, ensuring that the model does not have access to information from the test set during training.\n",
    "\n",
    "Purpose The purpose of this code is to prepare the text data for training a machine learning model by converting the textual information into a numerical format that can be fed into the model. TF-IDF is a popular method for representing text data, as it takes into account both the frequency of words in a document and their importance across the entire corpus.\n",
    "\n",
    "#### I) Training and Evaluation using different Models after augmentation\n",
    "\n",
    "##### 1) Logistic Regression Model\n",
    "\n",
    "alr = LogisticRegression(class_weight='balanced') Initializes the Logistic Regression Model\n",
    "\n",
    "alr.fit(X_train_tfvec,y_train) Fits the model on the training data\n",
    "\n",
    "y_pred_alr = alr.predict(X_test_tfvec) Obtains preditctions on the test data\n",
    "\n",
    "accuracy_score(y_test,y_pred_alr) Using accuracy_score() we check the accuracy on the testing dataset\n",
    "\n",
    "##### confusion matrix\n",
    "\n",
    "cmalr = confusion_matrix(y_test, y_pred_alr, labels=alr.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmalr, display_labels=alr.classes_)\n",
    "disp.plot()\n",
    "plt.savefig(os.path.join(visuals_folder_aa, \"LR_Confusion_Matrix.png\"))\n",
    "plt.show()\n",
    "\n",
    "cmalr = confusion_matrix(y_test, y_pred_alr, labels=alr.classes_) \n",
    "This computes the confusion matrix for your logistic regression model (alr). It compares the true labels (y_test) with the predicted labels (y_pred_alr).\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmalr, display_labels=alr.classes_) This creates a display object for the confusion matrix (disp), which will be plotted later. The matrix will have labels corresponding to the classes in your model.\n",
    "\n",
    "disp.plot() This generates a plot of the confusion matrix.\n",
    "\n",
    "plt.savefig(os.path.join(visuals_folder_aa, \"LR_Confusion_Matrix.png\")) Saves the confusion matrix plot as an image (LR_Confusion_Matrix.png) in the specified folder (visuals_folder_aa).\n",
    "\n",
    "plt.show() This shows the confusion matrix plot in a window or notebook, depending on your environment.\n",
    "\n",
    "#####\n",
    "\n",
    "print(classification_report(y_pred_alr,y_test)) evaluates the performance of logistic regression model on test data using Classification Report\n",
    "\n",
    "update_score_card(y_test,y_pred_alr,'Initial_Augmented_LR_model') calls the update score card method to update the score card with this model's score\n",
    "\n",
    "##### Performing Hyperparameter tuning\n",
    "\n",
    "param_gridalr = { 'C': [0.01, 0.1, 1, 10, 100], 'solver': ['liblinear', 'newton-cg', 'saga'] } defines the hyperparameter grid dictionary\n",
    "\n",
    "grid_searchalr = GridSearchCV(alr, param_gridlr, cv=5, scoring='accuracy') \n",
    "\n",
    "This creates a GridSearchCV object that will: \n",
    "Perform cross-validation with cv=5 (5-fold cross-validation) to evaluate the model’s performance. Optimize accuracy using scoring='accuracy'. \n",
    "Search over the parameter grid param_gridlr to find the best combination of C and solver for the Logistic Regression model (alr).\n",
    "\n",
    "grid_searchalr.fit(X_train_tfvec, y_train) Fits the grid on the training data\n",
    "\n",
    "best_alr = grid_searchalr.best_estimator_ : Obtains the Best model and stores it in best_alr\n",
    "\n",
    "best_alr - checks the best model \n",
    "\n",
    "best_alr.fit(X_train_tfvec, y_train) - Fits the best model obtained on the training data \n",
    "\n",
    "y_pred_balr = best_alr.predict(X_test_tfvec) - Obtains preditctions on the test data\n",
    "\n",
    "accuracy_score(y_test,y_pred_balr) : Using accuracy_score() we check the accuracy on the testing dataset\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "cmbalr = confusion_matrix(y_test, y_pred_balr, labels=best_alr.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmbalr, display_labels=best_alr.classes_)\n",
    "disp.plot()\n",
    "plt.savefig(os.path.join(visuals_folder_aa, \"Tuned_LR_Confusion_Matrix.png\"))\n",
    "plt.show()\n",
    "\n",
    "This code snippet plots a confusion matrix for the tuned LR Model. The code does the same thing as the previous confusion matrix code except that it Saves the confusion matrix plot as an image (Tuned_LR_Confusion_Matrix.png) in the specified folder (visuals_folder_aa).\n",
    "\n",
    "#####\n",
    "\n",
    "print(classification_report(y_pred_balr,y_test)) evaluates the performance of the tuned logistic regression model on test data using Classification Report\n",
    "\n",
    "update_score_card(y_test,y_pred_balr,'BestTuned_Augmented_LR_model') calls the update score card method to update the score card with this model's score\n",
    "\n",
    "##### 2) Random Forest Classifier Model\n",
    "\n",
    "arf = RandomForestClassifier(n_estimators=200, random_state=0) Initializes the Random Forest Classifier Model\n",
    "\n",
    "arf.fit(X_train_tfvec,y_train) Fits it on the training data\n",
    "\n",
    "y_pred_arf = arf.predict(X_test_tfvec) Obtains predictions on the test data\n",
    "\n",
    "accuracy_score(y_test,y_pred_arf) - Using accuracy_score() we check the accuracy on the testing dataset\n",
    "\n",
    "##### Confusion Matrix\n",
    "\n",
    "cmarf = confusion_matrix(y_test, y_pred_arf, labels=arf.classes_) \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmarf, display_labels=arf.classes_) \n",
    "disp.plot() \n",
    "plt.savefig(os.path.join(visuals_folder_aa, \"RFC_Confusion_Matrix.png\")) \n",
    "plt.show()\n",
    "\n",
    "This code snippet plots a confusion matrix for the RFC Model. The code does the same thing as the previous confusion matrix code except that it Saves the confusion matrix plot as an image (RFC_Confusion_Matrix.png) in the specified folder (visuals_folder_aa).\n",
    "\n",
    "#####\n",
    "\n",
    "print(classification_report(y_pred_arf,y_test)) evaluates the performance of the random forest classifier model on test data using classification report\n",
    "\n",
    "update_score_card(y_test,y_pred_arf,'Initial_Augmented_RFC_model') calling the update score card method to update the score card with this model's score\n",
    "\n",
    "##### 3) Support Vector Machine Classifier Model\n",
    "\n",
    "aSVC = LinearSVC() Initializes the Support Vector Machine Classifier\n",
    "\n",
    "aSVC.fit(X_train_tfvec,y_train) Fits the model on the training data\n",
    "\n",
    "y_pred_aSVC = aSVC.predict(X_test_tfvec) Obtains the predictions on the test data\n",
    "\n",
    "accuracy_score(y_test,y_pred_aSVC) - Using accuracy_score() we check the accuracy on the testing dataset\n",
    "\n",
    "##### Confusion Matrix\n",
    "\n",
    "cmasvc = confusion_matrix(y_test, y_pred_aSVC, labels=aSVC.classes_) \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmasvc, display_labels=aSVC.classes_) \n",
    "disp.plot() \n",
    "plt.savefig(os.path.join(visuals_folder_aa, \"SVC_Confusion_Matrix.png\")) \n",
    "plt.show()\n",
    "\n",
    "This code snippet plots a confusion matrix for the SVC Model. The code does the same thing as the previous confusion matrix code except that it Saves the confusion matrix plot as an image (SVC_Confusion_Matrix.png) in the specified folder (visuals_folder_aa).\n",
    "\n",
    "#####\n",
    "\n",
    "print(classification_report(y_pred_aSVC,y_test)) evaluates the performance of the Support Vector Classifier on test data using Classification Report\n",
    "\n",
    "update_score_card(y_test,y_pred_aSVC,'Initial_Augmented_SVC_model') calls the update score card method to update the score card with this model's score\n",
    "\n",
    "##### Performing Hyperparameter tuning\n",
    "\n",
    "param_grid_asvc = { 'C': [0.01, 0.1, 1, 10, 100] } defines the hyperparameter grid dictionary\n",
    "\n",
    "grid_search_asvc = GridSearchCV(aSVC, param_grid_asvc, cv=5, scoring='accuracy')\n",
    "\n",
    "This creates a GridSearchCV object that will: \n",
    "Perform 5-fold cross-validation (cv=5) to evaluate the Support Vector Classifier (SVC) model's performance. \n",
    "Optimize based on accuracy using scoring='accuracy'. \n",
    "Search through the hyperparameter grid (param_grid_svc) to find the best combination of parameters for the SVC model.\n",
    "\n",
    "grid_search_asvc.fit(X_train_tfvec, y_train) Fits the grid on the training data\n",
    "\n",
    "best_asvc = grid_search_asvc.best_estimator_ \n",
    "print(f\"Best parameters: {grid_search_asvc.best_params_}\") \n",
    "print(f\"Best cross-validation accuracy: {grid_search_asvc.best_score_}\")\n",
    "\n",
    "This snippet Gets and prints the best model and parameters\n",
    "\n",
    "best_asvc - checks the best model \n",
    "\n",
    "best_asvc.fit(X_train_tfvec, y_train) - Fits the best model obtained on the training data\n",
    "\n",
    "y_pred_basvc = best_asvc.predict(X_test_tfvec) - Obtains the predictions on the test data\n",
    "\n",
    "accuracy_score(y_test,y_pred_basvc) - Using accuracy_score() we check the accuracy on the testing dataset\n",
    "\n",
    "##### Confusion Matrix\n",
    "\n",
    "cmbasvc = confusion_matrix(y_test, y_pred_basvc, labels=best_asvc.classes_) \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmbasvc, display_labels=best_asvc.classes_) \n",
    "disp.plot() \n",
    "plt.savefig(os.path.join(visuals_folder_aa, \"Tuned_SVC_Confusion_Matrix.png\")) \n",
    "plt.show()\n",
    "\n",
    "This code snippet plots a confusion matrix for the Tuned SVC Model. The code does the same thing as the previous confusion matrix code except that it saves the confusion matrix plot as an image (Tuned_SVC_Confusion_Matrix.png) in the specified folder (visuals_folder_aa).\n",
    "\n",
    "print(classification_report(y_pred_basvc,y_test)) evaluate the performance of the tuned Support Vector Classifier model on test data using Classification Report\n",
    "\n",
    "update_score_card(y_test,y_pred_basvc,'BestTuned_Augmented_SVC_model') calls the update score card method to update the score card with this model's score\n",
    "\n",
    "#### J) Deep Learning Approach¶\n",
    "\n",
    "##### Importing tensorflow and keras libraries for Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "\n",
    "##### A] LSTM (Long Short Term Memory) Model\n",
    "\n",
    "Performing Tokenization on a column of text data using the Keras Tokenizer\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "\n",
    "num_words=10000: Limits the tokenizer to the top 10,000 most frequent words in the dataset.\n",
    "oov_token=\"<OOV>\": Specifies a token (\"<OOV>\") to represent out-of-vocabulary words—words \n",
    "that were not in the top 10,000 during training.\n",
    "\n",
    "tokenizer.fit_on_texts(final_df['clean_content'])\n",
    "This method processes the text data passed (df['clean_content']) and builds a vocabulary index based on word frequency.\n",
    "\n",
    "After this step, the tokenizer will have a dictionary mapping each word in the vocabulary to an integer index.\n",
    "\n",
    "X_sequences = tokenizer.texts_to_sequences(final_df['clean_content'])\n",
    "\n",
    "This line converts the text data into a sequence of integers.\n",
    "The tokenizer is a Tokenizer object from Keras, which has been trained to convert words to integer indices (usually based on word frequency).\n",
    "texts_to_sequences() takes the cleaned text from the column clean_content of final_df and maps each word in the text \n",
    "to its corresponding index in the tokenizer’s vocabulary.\n",
    "\n",
    "The result, X_sequences, is a list of lists, where each sublist contains the integer sequence corresponding to a \n",
    "particular review or document in final_df.\n",
    "    \n",
    "X_padded = pad_sequences(X_sequences, maxlen=100, padding='post', truncating='post')\n",
    "\n",
    "This line ensures that all the sequences have the same length.\n",
    "pad_sequences() pads or truncates each sequence in X_sequences to a fixed length of 100.\n",
    "The argument padding='post' adds zeros at the end of sequences that are shorter than 100.\n",
    "The argument truncating='post' truncates sequences that are longer than 100 from the end.\n",
    "The result, X_padded, is a 2D array where each row corresponds to a padded sequence of length 100.\n",
    "\n",
    "This code is converting text data into a numerical format (sequences of word indices) and ensuring all sequences\n",
    "are of the same length by padding or truncating as needed.\n",
    "    \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder - imports LabelEncoder\n",
    "\n",
    "Convert labels to categorical format if necessary\n",
    "label_encoder = LabelEncoder() initializes an instance of the LabelEncoder class from Scikit-learn.\n",
    "\n",
    "LabelEncoder is used to convert categorical labels into a numerical format.\n",
    "It is often used when you have target labels (i.e., the y variable) that are \n",
    "categorical (e.g., \"positive,\" \"neutral,\" \"negative\") and you need to convert them into numerical values (e.g., 0, 1, 2) \n",
    "for machine learning models.\n",
    "    \n",
    "y = label_encoder.fit_transform(final_df['intensity'])\n",
    "\n",
    "The fit_transform() method does two things:\n",
    "Fit: It learns the unique classes from the intensity column and assigns each class a unique integer.\n",
    "Transform: It converts each category in the intensity column into its corresponding integer representation.\n",
    "\n",
    "The encoded labels are assigned to the variable y. This will be a NumPy array where each entry is an integer \n",
    "representing the corresponding intensity label.\n",
    "    \n",
    "X_padded.shape - checks the shape of X_padded\n",
    "    \n",
    "X_padded - checks X-padded\n",
    "    \n",
    "y.shape - checks the shape of y\n",
    "    \n",
    "y - checks y\n",
    "    \n",
    "lstm_df = pd.DataFrame(np.column_stack((X_padded, y)))\n",
    "Creates a new dataframe by combining X_padded & y\n",
    "    \n",
    "The line lstm_df = pd.DataFrame(np.column_stack((X_padded, y))) is used to create a new pandas DataFrame, \n",
    "lstm_df, by combining the X_padded data (your padded sequences) and the y data (your encoded target labels). \n",
    "\n",
    "np.column_stack((X_padded, y)):\n",
    "This function from NumPy horizontally stacks arrays or sequences of arrays.\n",
    "X_padded is a 2D array of the padded sequences (with shape (num_samples, 100)).\n",
    "y is the array of labels (with shape (num_samples,)).\n",
    "np.column_stack combines them into a new 2D array where y becomes an additional column, resulting in a shape of \n",
    "(num_samples, 101). Each row will contain 100 values from X_padded followed by the corresponding y label.\n",
    "\n",
    "pd.DataFrame():\n",
    "\n",
    "This converts the resulting NumPy array into a pandas DataFrame.\n",
    "Each row in lstm_df will contain the padded sequence for a specific text review (from X_padded) and the \n",
    "corresponding intensity label (from y).\n",
    "    \n",
    "lstm_df.head() - checks lstm_df\n",
    "    \n",
    "file_path = os.path.join(\"C:/Users/nikde/Documents/UpGrad/intensityanalysis/data/processeddata\", \"lstm_data.csv\")\n",
    "Specifies the file path where you want to save the file\n",
    "    \n",
    "lstm_df.to_csv(file_path, index=False)  \n",
    "Saves the lstm_df dataframe as a CSV file\n",
    "Sets index=False to avoid saving the DataFrame index\n",
    "    \n",
    "##### Performing train test Split for LSTM¶\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "splits the data into training and test set with balanced split based on target class\n",
    "    \n",
    "##### Build LSTM Model\n",
    "    \n",
    "lstm_model1 = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=128, input_length=100),\n",
    "    Bidirectional(LSTM(64, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # 3 output classes\n",
    "])\n",
    "\n",
    "This code snippet defines a Long Short-Term Memory (LSTM) model using Keras' Sequential API for text classification\n",
    "\n",
    "i) Sequential():\n",
    "\n",
    "This creates a linear stack of layers for your neural network. Each layer feeds into the next.\n",
    "\n",
    "ii) Embedding(input_dim=10000, output_dim=128, input_length=100):\n",
    "\n",
    "This layer converts integer-encoded words into dense vectors of fixed size (in this case, 128).\n",
    "\n",
    "input_dim=10000: The size of the vocabulary (the maximum number of unique tokens/words). \n",
    "Here, it means the model can handle up to 10,000 different words.\n",
    "\n",
    "output_dim=128: The dimensionality of the dense embedding output for each word.\n",
    "\n",
    "input_length=100: This specifies that each input sequence has a length of 100.\n",
    "\n",
    "iii) Bidirectional(LSTM(64, return_sequences=False)):\n",
    "\n",
    "This adds a Bidirectional LSTM layer with 64 units.\n",
    "\n",
    "Bidirectional: This allows the model to learn patterns in both directions (forward and backward) in the input sequence, \n",
    "which can be beneficial for understanding context.\n",
    "\n",
    "LSTM(64): Specifies that the LSTM layer will have 64 memory units (or hidden states).\n",
    "\n",
    "return_sequences=False: This means that the LSTM will only return the output of the last time step \n",
    "(the final output of the sequence) instead of returning the output for every time step. \n",
    "\n",
    "iv) Dropout(0.5):\n",
    "\n",
    "This layer helps prevent overfitting by randomly setting 50% of the inputs to zero during training, \n",
    "which encourages the model to learn more robust features.\n",
    "\n",
    "v) Dense(64, activation='relu'):\n",
    "\n",
    "A fully connected (dense) layer with 64 units and ReLU (Rectified Linear Unit) activation function. \n",
    "This layer can help the model learn complex representations.\n",
    "\n",
    "vi) Dense(3, activation='softmax'):\n",
    "\n",
    "This is the output layer with 3 units (for three classes) and a softmax activation function.\n",
    "\n",
    "activation='softmax': This ensures that the output is a probability distribution over the three classes, \n",
    "where the sum of the probabilities equals 1. Each output corresponds to the likelihood of the input belonging to each class.\n",
    "\n",
    "##### Compile the LSTM Model\n",
    "    \n",
    "lstm_model1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "This line is used to configure the LSTM model for training\n",
    "\n",
    "loss='sparse_categorical_crossentropy':\n",
    "\n",
    "This specifies the loss function to be used during training.\n",
    "sparse_categorical_crossentropy is appropriate for multi-class classification problems where the target labels are integers.\n",
    "\n",
    "This loss function computes the cross-entropy loss between the predicted probability distribution \n",
    "(from the softmax output layer) and the true labels, which helps the model learn by minimizing this loss.\n",
    "\n",
    "optimizer='adam':\n",
    "\n",
    "This specifies the optimization algorithm used for training the model.\n",
    "adam (Adaptive Moment Estimation) is a popular optimizer that adjusts the learning rate for each parameter\n",
    "based on first and second moments of the gradients. It generally works well in practice and often leads to faster convergence.\n",
    "\n",
    "metrics=['accuracy']:\n",
    "\n",
    "This specifies the metrics to evaluate the model's performance during training and testing.\n",
    "accuracy is a common metric for classification tasks, measuring the ratio of correctly predicted samples to the total samples.\n",
    "    \n",
    "    \n",
    "lstm_model1 - checks the lstm_model1\n",
    "    \n",
    "lstm_model1.summary() - this generates a summary of the LSTM model architecture, providing an overview of the layers, \n",
    "their output shapes, and the number of parameters in the model\n",
    "\n",
    "##### Training the LSTM Model    \n",
    "\n",
    "history = lstm_model1.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)\n",
    "\n",
    "this line of code trains the LSTM model on the training data for 10 epochs, using a batch size of 32, \n",
    "while also evaluating its performance on a separate validation dataset. \n",
    "\n",
    "The training history will help you analyze the model’s performance and potentially fine-tune hyperparameters or \n",
    "architecture later.\n",
    "    \n",
    "lstm_model1.fit(...):\n",
    "\n",
    "This method trains the model using the provided training data.\n",
    "\n",
    "X_train:\n",
    "\n",
    "This is the input data for training, which should be a 2D array of shape (num_samples, 100), where each row corresponds to a padded sequence of text data.\n",
    "\n",
    "y_train:\n",
    "\n",
    "This is the target data for training, which should be a 1D array containing the encoded class labels corresponding to each sample in X_train.\n",
    "\n",
    "epochs=10:\n",
    "\n",
    "This specifies the number of times the entire training dataset will be passed forward and backward through the model. In this case, the model will train for 10 epochs.\n",
    "\n",
    "validation_data=(X_test, y_test):\n",
    "\n",
    "This argument provides validation data to evaluate the model’s performance after each epoch.\n",
    "\n",
    "X_test is the input data for validation, and y_test is the corresponding target labels. This allows you to monitor the model's performance on unseen data during training.\n",
    "\n",
    "batch_size=32:\n",
    "\n",
    "This defines the number of samples processed before the model is updated. In this case, the model will update its weights after processing every 32 samples. Smaller batch sizes can lead to more noisy updates, while larger batch sizes can smooth out the updates but may require more memory.\n",
    "\n",
    "history =:\n",
    "\n",
    "This stores the output of the fit() method, which contains the training and validation metrics for each epoch, such as loss and accuracy. You can use this history object to visualize the training process and evaluate the model's performance over time.\n",
    "\n",
    "lstm_model1.summary() - Prints the summary of the lstm_model1 after fittting on the training data\n",
    "\n",
    "##### Evaluating the model\n",
    "    \n",
    "test_loss, test_acc = lstm_model1.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc}\") - This line prints the test accuracy to the console\n",
    "    \n",
    "lstm_model1.evaluate(X_test, y_test):\n",
    "\n",
    "The evaluate method calculates the loss and any specified metrics (like accuracy) for the model using the provided test data. \n",
    "\n",
    "X_test: This is the input data for testing, which should be a 2D array of padded sequences. y_test: This is the corresponding target labels for the test data, which contains the encoded class labels.\n",
    "\n",
    "test_loss, test_acc =:\n",
    "\n",
    "The evaluate method returns two values:\n",
    "\n",
    "test_loss: This is the value of the loss function calculated on the test data. It indicates how well the model performs in terms of its objective (in this case, minimizing sparse categorical cross-entropy).\n",
    "\n",
    "test_acc: This is the accuracy of the model on the test data, showing the proportion of correctly predicted samples out of all test samples.\n",
    "\n",
    "y_pred_lstm = lstm_model1.predict(X_test)\n",
    "generate predictions from the trained LSTM model for the test dataset.\n",
    "    \n",
    "y_pred_lstm - checks the predictions\n",
    "    \n",
    "y_pred_lstm_labels = np.argmax(y_pred_lstm, axis=1) - Converts the predicted probabilities to class labels\n",
    "    \n",
    "np.argmax(y_pred_lstm, axis=1):\n",
    "\n",
    "The np.argmax() function from the NumPy library returns the indices of the maximum values along the specified axis.\n",
    "\n",
    "y_pred_lstm: This is the 2D array of predicted probabilities generated by the LSTM model for each class. axis=1: This specifies that the operation should be performed along the rows. In this case, it means that for each row (each sample), it will find the index of the highest probability across the columns (classes).\n",
    "\n",
    "y_pred_lstm_labels =:\n",
    "\n",
    "The result of the np.argmax() operation is assigned to the variable y_pred_lstm_labels. This will be a 1D array containing the class labels corresponding to the highest predicted probabilities for each sample in the test set.\n",
    "\n",
    "y_pred_lstm_labels - checks the class labels generated\n",
    "    \n",
    "accuracy_score(y_test,y_pred_lstm_labels) - Using accuracy_score() to check the accuracy on the testing dataset\n",
    "    \n",
    "np.unique(y_test) - checks the unique labels in y_test\n",
    "    \n",
    "##### Confusion Matrix¶\n",
    "    \n",
    "cmlstm = confusion_matrix(y_test,y_pred_lstm_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmlstm, display_labels=np.unique(y_test))\n",
    "disp.plot()\n",
    "plt.savefig(os.path.join(visuals_folder_aa, \"LSTM_Confusion_Matrix.png\"))\n",
    "plt.show()\n",
    "    \n",
    "This code snippet plots a confusion matrix for the LSTM Model. The code does the same thing as the previous confusion matrix code except that it saves the confusion matrix plot as an image (LSTM_Confusion_Matrix.png) in the specified folder (visuals_folder_aa).\n",
    "    \n",
    "#####\n",
    "    \n",
    "print(classification_report(y_test,y_pred_lstm_labels)) - evaluate the performance of the LSTM model on test data using Classification Report\n",
    "    \n",
    "update_score_card(y_test,y_pred_lstm_labels,'Initial_LSTM_model') - calls the update score card method to update the score card with this model's score\n",
    "    \n",
    "##### Performing Hyperparameter tuning\n",
    "    \n",
    "pip install keras-tuner - installs keras-tuner which needs to be installed for hyperparameter tuning\n",
    "    \n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import RandomSearch\n",
    "importing libraries required for hyperparameter tuning\n",
    "    \n",
    "Steps involved\n",
    "\n",
    "1) defining the hypermodel\n",
    "The function build_model is defined to create a Sequential model. It accepts a hp argument, which represents the hyperparameter space.\n",
    "\n",
    "Embedding Layer: The output dimension of the embedding layer is defined as a hyperparameter that can vary between 64 and 256 with a step of 32.\n",
    "\n",
    "LSTM Layer: The number of LSTM units is also defined as a hyperparameter between 32 and 256.\n",
    "\n",
    "Dropout Layer: The dropout rate is defined as a hyperparameter ranging from 0.2 to 0.5, which helps prevent overfitting.\n",
    "\n",
    "Dense Layer: The number of units in the dense layer is another hyperparameter, ranging from 32 to 128.\n",
    "\n",
    "Output Layer: The final layer uses a softmax activation function for multi-class classification (3 classes).\n",
    "\n",
    "Model Compilation: The model is compiled using Adam optimizer with a learning rate defined as a hyperparameter, sparse categorical cross-entropy loss, and accuracy as a metric.\n",
    "\n",
    "2) Instantiating the Tuner\n",
    "Tuner initialization: A RandomSearch tuner is instantiated, which will search for the best hyperparameters.\n",
    "\n",
    "Parameters:\n",
    "\n",
    "objective='val_accuracy': The tuner aims to maximize validation accuracy.\n",
    "\n",
    "max_trials=5: The tuner will try 5 different combinations of hyperparameters.\n",
    "\n",
    "executions_per_trial=1: For each trial, one model will be built and evaluated.\n",
    "\n",
    "directory and project_name: These specify where to save the tuning results.\n",
    "\n",
    "3) Search for the Best Hyperparameters\n",
    "Hyperparameter Search: The search method runs the random search to find the best hyperparameters based on the training and validation data. It trains each model for 10 epochs using a batch size of 32.\n",
    "\n",
    "4) Get the Best Model\n",
    "Retrieve Best Hyperparameters: The best hyperparameters found during the search are stored in best_hps.\n",
    "\n",
    "Build Best Model: The best model is created using the hyperparameters retrieved.\n",
    "\n",
    "5) Train the Best Model\n",
    "Training: The best-tuned model is trained on the training dataset for 10 epochs, using the same validation data.\n",
    "\n",
    "6) Evaluate the Best Model\n",
    "Model Evaluation: The model is evaluated on the test dataset, returning the test loss and accuracy.\n",
    "\n",
    "Print Accuracy: The accuracy of the best model on the test data is printed\n",
    "    \n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "Define the hypermodel\n",
    "def build_model(hp):\n",
    "    lstm_tuned_model = Sequential()\n",
    "\n",
    "    Embedding layer\n",
    "    lstm_tuned_model.add(Embedding(input_dim=10000,\n",
    "                        output_dim=hp.Int('embedding_output', min_value=64, max_value=256, step=32),\n",
    "                        input_length=100))\n",
    "\n",
    "    LSTM layer\n",
    "    lstm_tuned_model.add(Bidirectional(LSTM(units=hp.Int('lstm_units', min_value=32, max_value=256, step=32), return_sequences=False)))\n",
    "\n",
    "    Dropout layer\n",
    "    lstm_tuned_model.add(Dropout(rate=hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "    Dense layer\n",
    "    lstm_tuned_model.add(Dense(units=hp.Int('dense_units', min_value=32, max_value=128, step=32), activation='relu'))\n",
    "\n",
    "    Output layer\n",
    "    lstm_tuned_model.add(Dense(3, activation='softmax'))  # 3 output classes\n",
    "\n",
    "    Compile the model\n",
    "    lstm_tuned_model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "                      learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return lstm_tuned_model\n",
    "\n",
    "Instantiate the tuner\n",
    "lstmtuner1 = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,  # Number of different hyperparameter combinations to try\n",
    "    executions_per_trial=1,  # Number of models to build and evaluate for each trial\n",
    "    directory='lstm_tuning',\n",
    "    project_name='text_intensity_lstm')\n",
    "\n",
    "Search for the best hyperparameters\n",
    "lstmtuner1.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)\n",
    "\n",
    "Get the best model\n",
    "best_hps = lstmtuner1.get_best_hyperparameters()[0]\n",
    "best_tuned_lstm_model = lstmtuner1.hypermodel.build(best_hps)\n",
    "\n",
    "Train the best model\n",
    "history = best_tuned_lstm_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)\n",
    "\n",
    "Evaluate the best model\n",
    "test_loss, test_acc = best_tuned_lstm_model.evaluate(X_test, y_test)\n",
    "print(f\"Best Model Test Accuracy: {test_acc}\")\n",
    "    \n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "   \n",
    "best_tuned_lstm_model - checks the best tuned model\n",
    "    \n",
    "best_tuned_lstm_model.summary() - Prints the summary of the best_tuned_lstm_model after fittting on the training data\n",
    "    \n",
    "y_pred_btlstm = best_tuned_lstm_model.predict(X_test) - generate predictions from the best tuned LSTM model for the test dataset.\n",
    "    \n",
    "y_pred_btlstm_labels = np.argmax(y_pred_btlstm, axis=1) - Convert the predicted probabilities to class labels\n",
    "    \n",
    "accuracy_score(y_test,y_pred_btlstm_labels) - Using accuracy_score() we check the accuracy on the testing dataset\n",
    "\n",
    "##### Confusion matrix\n",
    "    \n",
    "cmbtlstm = confusion_matrix(y_test,y_pred_btlstm_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmbtlstm, display_labels=np.unique(y_test))\n",
    "disp.plot()\n",
    "plt.savefig(os.path.join(visuals_folder_aa, \"Tuned_LSTM_Confusion_Matrix.png\"))\n",
    "plt.show()\n",
    "    \n",
    "This code snippet plots a confusion matrix for the Tuned LSTM Model. The code does the same thing as the previous confusion matrix code except that it saves the confusion matrix plot as an image (Tuned_LSTM_Confusion_Matrix.png) in the specified folder (visuals_folder_aa).\n",
    "    \n",
    "##### \n",
    "    \n",
    "print(classification_report(y_test,y_pred_btlstm_labels))\n",
    "evaluates the performance of the Tuned LSTM model on test data using Classification Report\n",
    "    \n",
    "update_score_card(y_test,y_pred_btlstm_labels,'Tuned_LSTM_model')\n",
    "calls the update score card method to update the score card with this model's score\n",
    "    \n",
    "##### Building a hypermodel with a deeper architure by adding another bi-directional layer\n",
    "    \n",
    "Key Changes made\n",
    "Multiple LSTM Layers:\n",
    "\n",
    "Added a second LSTM layer with return_sequences=False to stop sequence propagation after the second layer.\n",
    "\n",
    "Hyperparameter Tuning for Multiple Layers:\n",
    "\n",
    "Hyperparameters are now tuned for both LSTM layers (lstm_units_1 and lstm_units_2). Separate dropout rates for each layer (dropout_rate_1, dropout_rate_2), including a dense layer dropout (dropout_rate_dense).\n",
    "\n",
    "Additional Dense Layer:\n",
    "\n",
    "A deeper Dense layer before the output has been added to introduce more non-linearity to the model.\n",
    "    \n",
    "-------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "Define the hypermodel with deeper architecture\n",
    "def build_model(hp):\n",
    "    deeper_lstm_model = Sequential()\n",
    "\n",
    "    Embedding layer\n",
    "    deeper_lstm_model.add(Embedding(input_dim=10000,\n",
    "                        output_dim=hp.Int('embedding_output', min_value=64, max_value=256, step=32),\n",
    "                        input_length=100))\n",
    "\n",
    "    First LSTM layer (stacked)\n",
    "    deeper_lstm_model.add(Bidirectional(LSTM(units=hp.Int('lstm_units_1', min_value=32, max_value=256, step=32), return_sequences=True)))\n",
    "    deeper_lstm_model.add(Dropout(rate=hp.Float('dropout_rate_1', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "    Second LSTM layer (stacked)\n",
    "    deeper_lstm_model.add(Bidirectional(LSTM(units=hp.Int('lstm_units_2', min_value=32, max_value=256, step=32), return_sequences=False)))\n",
    "    deeper_lstm_model.add(Dropout(rate=hp.Float('dropout_rate_2', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "    Dense layer\n",
    "    deeper_lstm_model.add(Dense(units=hp.Int('dense_units', min_value=32, max_value=128, step=32), activation='relu'))\n",
    "    deeper_lstm_model.add(Dropout(rate=hp.Float('dropout_rate_dense', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "\n",
    "    Output layer (3 output classes)\n",
    "    deeper_lstm_model.add(Dense(3, activation='softmax'))  # For 3 classes: 'angriness', 'happiness', 'sadness'\n",
    "\n",
    "    Compile the model\n",
    "    deeper_lstm_model.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "                      learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return deeper_lstm_model\n",
    "\n",
    "Instantiate the tuner\n",
    "lstmtuner2 = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,  # Number of different hyperparameter combinations to try\n",
    "    executions_per_trial=1,  # Number of models to build and evaluate for each trial\n",
    "    directory='lstm_tuning',\n",
    "    project_name='text_intensity_lstm')\n",
    "\n",
    "Search for the best hyperparameters\n",
    "lstmtuner2.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)\n",
    "\n",
    "Get the best model\n",
    "best_hps = lstmtuner2.get_best_hyperparameters()[0]\n",
    "best_deep_lstm_model = lstmtuner2.hypermodel.build(best_hps)\n",
    "\n",
    "Train the best model\n",
    "history = best_deep_lstm_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), batch_size=32)\n",
    "\n",
    "Evaluate the best model\n",
    "test_loss, test_acc = best_deep_lstm_model.evaluate(X_test, y_test)\n",
    "print(f\"Best Model Test Accuracy: {test_acc}\")\n",
    "    \n",
    "-----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "best_deep_lstm_model - checks the best deeper tuned LSTM Model\n",
    "    \n",
    "best_deep_lstm_model.summary() - Prints the summary of the best_deep_tuned_lstm_model after fittting on the training data\n",
    "    \n",
    "y_pred_dtlstm = best_deep_lstm_model.predict(X_test) - generates predictions from the best deeper tuned LSTM model for the test dataset.\n",
    "    \n",
    "y_pred_dtlstm_labels = np.argmax(y_pred_dtlstm, axis=1) - Converts the predicted probabilities to class labels\n",
    "    \n",
    "accuracy_score(y_test,y_pred_dtlstm_labels) - Using accuracy_score() we are check the accuracy on the testing dataset\n",
    "    \n",
    "##### Confusion Matrix\n",
    "    \n",
    "cmdtlstm = confusion_matrix(y_test,y_pred_dtlstm_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmdtlstm, display_labels=np.unique(y_test))\n",
    "disp.plot()\n",
    "plt.savefig(os.path.join(visuals_folder_aa, \"Deeper_Tuned_LSTM_Confusion_Matrix.png\"))\n",
    "plt.show()\n",
    "    \n",
    "This code snippet plots a confusion matrix for the Deeper Tuned LSTM Model. The code does the same thing as the previous confusion matrix code except that it saves the confusion matrix plot as an image (Deeper_Tuned_LSTM_Confusion_Matrix.png) in the specified folder (visuals_folder_aa).\n",
    "    \n",
    "##### \n",
    "    \n",
    "print(classification_report(y_test,y_pred_dtlstm_labels)) \n",
    "evaluates the performance of the Deeper Tuned LSTM model on test data using Classification Report\n",
    "\n",
    "update_score_card(y_test,y_pred_dtlstm_labels,'Deeper_Tuned_LSTM_model')\n",
    "calls the update score card method to update the score card with this model's score\n",
    "    \n",
    "##### B] CNN (Convolutional Neural Network Model)\n",
    "    \n",
    "importing libraries required for CNN\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Embedding, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "    \n",
    "Defining constants\n",
    "MAX_VOCAB_SIZE = 10000  - Vocabulary size\n",
    "MAX_SEQUENCE_LENGTH = 100  - Maximum length of sequences\n",
    "EMBEDDING_DIM = 100  - Dimensionality of the embedding layer\n",
    "NUM_CLASSES = 3   -Adjust according to your number of classes\n",
    "    \n",
    "Extracting texts and labels from final_df\n",
    "texts = final_df['clean_content'].values\n",
    "labels = final_df['intensity'].values\n",
    "    \n",
    "cnn_df = pd.DataFrame(np.column_stack((texts, labels)), columns=['CleanContent', 'Intensity'])\n",
    "Concatenates/Stacks texts and labels to create a dataframe for CNN Model\n",
    "    \n",
    "file_path = os.path.join(\"C:/Users/nikde/Documents/UpGrad/intensityanalysis/data/processeddata\", \"cnn_data.csv\")\n",
    "Specifies the file path where you want to save the file\n",
    "\n",
    "cnn_df.to_csv(file_path, index=False) \n",
    "Saves the cnn_df dataframe as a CSV file\n",
    "Sets index=False to avoid saving the DataFrame index\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "Splits the data into training and testing sets\n",
    "    \n",
    "Performing Tokenization on a column of text data using the Keras Tokenizer\n",
    "    \n",
    "tokenizer = Tokenizer(num_words=10000)  # Set the maximum vocabulary size\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "This method processes the text data passed (df['clean_content']) and builds a vocabulary index based on word frequency.\n",
    "After this step, the tokenizer will have a dictionary mapping each word in the vocabulary to an integer index.\n",
    "    \n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "Converts text to sequences\n",
    "    \n",
    "Padding the sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100  # Set your desired max length\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "Since the input sequences need to be of the same length, you use pad_sequences to pad (or truncate) them to a fixed length, \n",
    "defined by MAX_SEQUENCE_LENGTH. In this case, the sequences are either truncated or padded to a length of 100.\n",
    "    \n",
    "Encoding labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "You use LabelEncoder to convert the categorical labels (y_train and y_test) into integer-encoded values. \n",
    "The fit_transform method is used on the training labels, while transform is used on the test labels to ensure \n",
    "consistent encoding.\n",
    "\n",
    "The texts_to_sequences method converts each sentence in the training and testing datasets into a sequence of integers, \n",
    "where each integer represents a word in the tokenizer's vocabulary.\n",
    "    \n",
    "cnn_df - checks cnn_df\n",
    "    \n",
    "print(\"Training data shape:\", X_train_padded.shape, y_train_encoded.shape)\n",
    "print(\"Testing data shape:\", X_test_padded.shape, y_test_encoded.shape)\n",
    "    \n",
    "checks and prints the shape of X_train_padded, y_train_encoded, X_test_padded, y_test_encoded\n",
    "    \n",
    "-----------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "Defining a Function to Build and Compile the CNN Model\n",
    "    \n",
    "def create_cnn_model(vocabulary_size, embedding_dim, max_length):\n",
    "    cnnmodel = Sequential()\n",
    "\n",
    "    Embedding layer\n",
    "    cnnmodel.add(Embedding(input_dim=vocabulary_size,\n",
    "                        output_dim=embedding_dim,\n",
    "                        input_length=max_length))\n",
    "\n",
    "    Convolutional layer\n",
    "    cnnmodel.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "\n",
    "    Max pooling layer\n",
    "    cnnmodel.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "    Flatten layer\n",
    "    cnnmodel.add(Flatten())\n",
    "\n",
    "    Fully connected layer\n",
    "    cnnmodel.add(Dense(128, activation='relu'))\n",
    "    cnnmodel.add(Dropout(0.5))\n",
    "\n",
    "    Output layer\n",
    "    cnnmodel.add(Dense(3, activation='softmax'))  # Change '3' to the number of classes in your dataset\n",
    "\n",
    "    Compile the model\n",
    "    cnnmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return cnnmodel\n",
    "    \n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "This code defines a function create_cnn_model that builds and compiles a Convolutional Neural Network (CNN) model using the Keras Sequential API for text classification tasks. \n",
    " \n",
    "def create_cnn_model(vocabulary_size, embedding_dim, max_length):\n",
    "The function accepts three parameters:\n",
    "\n",
    "vocabulary_size: The size of the vocabulary (number of unique words) for embedding.\n",
    "embedding_dim: The dimensionality of the dense word vectors (i.e., how each word will be represented in a vector space).\n",
    "max_length: The maximum length of input sequences (used to pad/truncate sequences to this length).\n",
    "    \n",
    "cnnmodel = Sequential()\n",
    "A Sequential model allows you to build a neural network layer by layer, where each layer has one input tensor and one output tensor.\n",
    "    \n",
    "cnnmodel.add(Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=max_length))\n",
    "\n",
    "Embedding Layer: Converts words into dense vectors of fixed size (embedding_dim).\n",
    "input_dim=vocabulary_size: The size of the vocabulary (number of words).\n",
    "output_dim=embedding_dim: The size of each embedding vector.\n",
    "input_length=max_length: Each input sentence will be padded/truncated to this length.\n",
    "    \n",
    "cnnmodel.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "    \n",
    "Conv1D Layer: Applies 1D convolutions over the input sequences (useful for detecting local patterns in text).\n",
    "filters=128: The number of output filters in the convolution.\n",
    "kernel_size=5: The size of the convolutional window (i.e., looks at 5 adjacent words at a time).\n",
    "activation='relu': The ReLU activation function is applied, introducing non-linearity.\n",
    "    \n",
    "cnnmodel.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "MaxPooling1D Layer: Reduces the dimensionality by selecting the maximum value over a window (helps retain important features while reducing computation).\n",
    "pool_size=2: The size of the pooling window (i.e., reduces the sequence length by half).\n",
    "    \n",
    "cnnmodel.add(Flatten())\n",
    "Flatten Layer: Converts the 3D output of the convolutional and pooling layers into a 1D vector (necessary for feeding into fully connected layers).\n",
    "    \n",
    "cnnmodel.add(Dense(128, activation='relu'))\n",
    "cnnmodel.add(Dropout(0.5))\n",
    "\n",
    "Dense Layer: A fully connected layer with 128 neurons and ReLU activation.\n",
    "Dropout Layer: Adds a dropout rate of 50% to prevent overfitting (randomly drops neurons during training).\n",
    "    \n",
    "cnnmodel.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "Dense Layer: The output layer with 3 neurons (assuming this is a 3-class classification problem).\n",
    "Softmax Activation: Used for multi-class classification, converts the output into a probability distribution.\n",
    "    \n",
    "cnnmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "Optimizer: Adam optimizer is used, which combines the advantages of RMSProp and Stochastic Gradient Descent.\n",
    "Loss: sparse_categorical_crossentropy is used for multi-class classification where the labels are integer-encoded (not one-hot encoded).\n",
    "Metrics: The model tracks accuracy during training and evaluation.\n",
    "    \n",
    "return cnnmodel\n",
    "The function returns the compiled CNN model.\n",
    "\n",
    "This CNN is designed for text data, where the embedding layer maps words to dense vectors, followed by a convolutional layer to capture patterns in the word sequences, and finally fully connected layers for classification.\n",
    "    \n",
    "Defining constants\n",
    "VOCABULARY_SIZE = 10000  Based on the tokenizer\n",
    "EMBEDDING_DIM = 128      You can choose the embedding dimension\n",
    "MAX_LENGTH = 100         Same as the max length used for padding\n",
    "    \n",
    "cnn_model = create_cnn_model(VOCABULARY_SIZE, EMBEDDING_DIM, MAX_LENGTH) - Creates the model by calling create_cnn_model by passing the constants defined as arguments\n",
    "\n",
    "Training the model\n",
    "history = cnn_model.fit(X_train_padded, y_train_encoded,\n",
    "                         epochs=10, batch_size=32,\n",
    "                         validation_split=0.2)\n",
    "    \n",
    "This piece of code is training the CNN model (cnn_model) using the .fit() method. Here's a breakdown:\n",
    "\n",
    "Explanation of Each Argument:\n",
    "\n",
    "X_train_padded: This is your input data for training. It likely contains padded sequences of text, with each sequence having a fixed length (max_length), as required by the CNN.\n",
    "\n",
    "y_train_encoded: This is the corresponding target/output labels for your training data, typically encoded as integers (for classification problems).\n",
    "\n",
    "Hyperparameters:\n",
    "epochs=10: The model will train for 10 complete passes through the entire training dataset. One epoch means training on the entire training set once.\n",
    "\n",
    "batch_size=32: The training data will be split into mini-batches of 32 samples each. The model will update its weights after each batch, making the training more efficient.\n",
    "\n",
    "Validation:\n",
    "validation_split=0.2: This indicates that 20% of the training data will be set aside as a validation set. During training, the model will evaluate its performance on this data (which is not used for training) to monitor overfitting and generalization to unseen data.\n",
    "\n",
    "Output:\n",
    "history: The result of the .fit() method is stored in the history object. This object contains training details, including loss and accuracy over epochs for both the training and validation sets. You can use this history to visualize the learning curves (e.g., loss and accuracy over time).\n",
    "\n",
    "In summary, this code trains the cnn_model on X_train_padded and y_train_encoded data for 10 epochs, with a batch size of 32, while also validating the model performance on 20% of the training data.\n",
    "    \n",
    "Evaluating the CNN Model\n",
    "\n",
    "test_loss, test_acc = cnn_model.evaluate(X_test_padded, y_test_encoded)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")\n",
    "- evaluates the model\n",
    "    \n",
    "cnn_model - Checks the CNN Model\n",
    "    \n",
    "cnn_model.summary() - Prints the summary of the CNN model after fittting on the training data\n",
    "    \n",
    "y_pred_cnn = cnn_model.predict(X_test_padded) - generates predictions from the CNN model for the test dataset.\n",
    "    \n",
    "y_pred_cnn_labels = np.argmax(y_pred_cnn, axis=1) - Converts the predicted probabilities to class labels\n",
    "    \n",
    "accuracy_score(y_test_encoded,y_pred_cnn_labels) - Using accuracy_score() we check the accuracy on the testing dataset\n",
    "    \n",
    "##### confusion matrix\n",
    "    \n",
    "cmcnn = confusion_matrix(y_test_encoded,y_pred_cnn_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmcnn, display_labels=np.unique(y_test_encoded))\n",
    "disp.plot()\n",
    "plt.savefig(os.path.join(visuals_folder_aa, \"CNN_Confusion_Matrix.png\"))\n",
    "plt.show()\n",
    "    \n",
    "This code snippet plots a confusion matrix for the CNN Model. The code does the same thing as the previous confusion matrix code except that it saves the confusion matrix plot as an image (CNN_Confusion_Matrix.png) in the specified folder (visuals_folder_aa).\n",
    "    \n",
    "#####\n",
    "    \n",
    "print(classification_report(y_test_encoded,y_pred_cnn_labels)) - evaluates the performance of the CNN model on test data using Classification Report\n",
    "    \n",
    "update_score_card(y_test_encoded,y_pred_cnn_labels,'Initial_CNN_model') - calls the update score card method to update the score card with this model's score\n",
    "    \n",
    "##### Performing Hyperparameter Tuning\n",
    "    \n",
    "---------------------------------------------------------------------------------------------------------------------------\n",
    "import kerastuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Embedding, Dropout\n",
    "\n",
    "def build_model(hp):\n",
    "    tcnnmodel = Sequential()\n",
    "    tcnnmodel.add(Embedding(input_dim=VOCABULARY_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LENGTH))\n",
    "    tcnnmodel.add(Conv1D(filters=hp.Int('filters', min_value=32, max_value=256, step=32), kernel_size=hp.Choice('kernel_size', values=[3, 5, 7]), activation='relu'))\n",
    "    tcnnmodel.add(MaxPooling1D(pool_size=2))\n",
    "    tcnnmodel.add(Flatten())\n",
    "    tcnnmodel.add(Dense(hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'))\n",
    "    tcnnmodel.add(Dropout(hp.Float('dropout_rate', min_value=0.3, max_value=0.7, step=0.1)))\n",
    "    tcnnmodel.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "    tcnnmodel.compile(optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop']), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return tcnnmodel\n",
    "\n",
    "cnntuner = kt.RandomSearch(build_model, objective='val_accuracy', max_trials=10, executions_per_trial=1, directory='my_dir', project_name='cnn_hyperparam_tuning')\n",
    "\n",
    "cnntuner.search(X_train_padded, y_train_encoded, epochs=10, validation_split=0.2)\n",
    "\n",
    "best_hps = cnntuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"Best hyperparameters: {best_hps.values}\")\n",
    "\n",
    "best_tcnnmodel = cnntuner.hypermodel.build(best_hps)\n",
    "best_tcnnmodel.summary()\n",
    "    \n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "def build_model(hp):\n",
    "    tcnnmodel = Sequential()\n",
    "\n",
    "This function defines a CNN model and uses the hp (hyperparameter) object provided by Keras Tuner to vary the hyperparameters.\n",
    "\n",
    "Embedding Layer:\n",
    "\n",
    "tcnnmodel.add(Embedding(input_dim=VOCABULARY_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_LENGTH))\n",
    "The embedding layer converts words into dense vectors, where VOCABULARY_SIZE, EMBEDDING_DIM, and MAX_LENGTH are predefined constants.\n",
    "\n",
    "Conv1D Layer with tunable hyperparameters:\n",
    "\n",
    "tcnnmodel.add(Conv1D(filters=hp.Int('filters', min_value=32, max_value=256, step=32), kernel_size=hp.Choice('kernel_size', values=[3, 5, 7]), activation='relu'))\n",
    "\n",
    "The hyperparameters for this layer are:\n",
    "\n",
    "filters: Tunable number of filters (32 to 256 in steps of 32).\n",
    "kernel_size: Tunable kernel size, which can be either 3, 5, or 7.\n",
    "\n",
    "MaxPooling Layer: \n",
    "tcnnmodel.add(MaxPooling1D(pool_size=2))\n",
    "The max pooling layer reduces the dimensionality of the feature maps.\n",
    "\n",
    "Flatten Layer:\n",
    "tcnnmodel.add(Flatten())\n",
    "The output is flattened into a 1D vector to feed into fully connected layers.\n",
    "\n",
    "Fully Connected (Dense) Layer with tunable units and dropout:\n",
    "\n",
    "tcnnmodel.add(Dense(hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'))\n",
    "tcnnmodel.add(Dropout(hp.Float('dropout_rate', min_value=0.3, max_value=0.7, step=0.1)))\n",
    "\n",
    "units: Tunable number of neurons (32 to 512 in steps of 32).\n",
    "dropout_rate: Tunable dropout rate (0.3 to 0.7 in steps of 0.1).\n",
    "\n",
    "Output Layer:\n",
    "\n",
    "tcnnmodel.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "A fully connected layer with NUM_CLASSES neurons, using the softmax activation for multi-class classification.\n",
    "\n",
    "Compilation:\n",
    "\n",
    "tcnnmodel.compile(optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop']), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "The optimizer is tunable, either 'adam' or 'rmsprop'.\n",
    "loss='sparse_categorical_crossentropy': Used for multi-class classification where the labels are integer-encoded.\n",
    "metrics=['accuracy']: The model tracks accuracy during training.\n",
    "    \n",
    "Hyperparameter Tuning: RandomSearch\n",
    "\n",
    "cnntuner = kt.RandomSearch(build_model, objective='val_accuracy', max_trials=10, executions_per_trial=1, directory='my_dir', project_name='cnn_hyperparam_tuning')\n",
    "\n",
    "kt.RandomSearch: A random search strategy for hyperparameter tuning.\n",
    "build_model: The model-building function where hyperparameters are tuned.\n",
    "objective='val_accuracy': The goal is to maximize validation accuracy.\n",
    "max_trials=10: The tuner will try 10 different hyperparameter combinations.\n",
    "executions_per_trial=1: Each trial (combination of hyperparameters) is run once.\n",
    "directory='my_dir': The directory where the results of the tuning process will be saved.\n",
    "project_name='cnn_hyperparam_tuning': The name of the project, used for organizing results.\n",
    "    \n",
    " \n",
    "Hyperparameter Search\n",
    "\n",
    "cnntuner.search(X_train_padded, y_train_encoded, epochs=10, validation_split=0.2)\n",
    "\n",
    "This command starts the hyperparameter search:\n",
    "X_train_padded and y_train_encoded are the training data and labels.\n",
    "epochs=10: Each model will be trained for 10 epochs.\n",
    "validation_split=0.2: 20% of the training data is set aside for validation.\n",
    "\n",
    "Best Hyperparameters\n",
    "\n",
    "best_hps = cnntuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best hyperparameters: {best_hps.values}\")\n",
    "\n",
    "get_best_hyperparameters(): This retrieves the best hyperparameter configuration after the search.\n",
    "num_trials=1: Retrieves the best combination (in this case, only one is needed).\n",
    "best_hps.values: Prints the best hyperparameters found.\n",
    "\n",
    "Building and Summarizing the Best Model\n",
    "best_tcnnmodel = cnntuner.hypermodel.build(best_hps)\n",
    "best_tcnnmodel.summary()\n",
    "\n",
    "best_tcnnmodel: This builds the best model using the optimal hyperparameters (best_hps).\n",
    "summary(): Prints a summary of the model architecture, including layer types and the number of parameters.\n",
    "\n",
    "Summary:\n",
    "This code defines a CNN for text classification and tunes hyperparameters such as the number of filters, kernel size, units in the dense layer, dropout rate, and optimizer.\n",
    "It uses Keras Tuner's RandomSearch to find the best set of hyperparameters by trying 10 different combinations and selecting the one with the highest validation accuracy.\n",
    " \n",
    "Training the best CNN Model obtained\n",
    "    \n",
    "history = best_tcnnmodel.fit(X_train_padded, y_train_encoded, \n",
    "                             epochs=10, \n",
    "                             validation_split=0.2)\n",
    "\n",
    "This piece of code is training the Best Tuned CNN model (best_tcnn_model) using the .fit() method. The explanation is the same as for training the initial CNN Model.\n",
    "    \n",
    "best_tcnnmodel - checks the best CNN Model\n",
    "    \n",
    "best_tcnnmodel.summary() - Prints the summary of the best tuned CNN model after fittting on the training data\n",
    "\n",
    "y_pred_tcnn = best_tcnnmodel.predict(X_test_padded) - generates predictions from the Tuned CNN model for the test dataset.\n",
    "    \n",
    "y_pred_tcnn_labels = np.argmax(y_pred_tcnn, axis=1) - Converts the predicted probabilities to class labels\n",
    "    \n",
    "accuracy_score(y_test_encoded,y_pred_tcnn_labels) - Using accuracy_score() we check the accuracy on the testing dataset\n",
    "    \n",
    "##### confusion matrix\n",
    "    \n",
    "cmtcnn = confusion_matrix(y_test_encoded,y_pred_tcnn_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cmtcnn, display_labels=np.unique(y_test_encoded))\n",
    "disp.plot()\n",
    "plt.savefig(os.path.join(visuals_folder_aa, \"Tuned_CNN_Confusion_Matrix.png\"))\n",
    "plt.show()\n",
    "    \n",
    "This code snippet plots a confusion matrix for the Tuned CNN Model. The code does the same thing as the previous confusion matrix code except that it saves the confusion matrix plot as an image (Tuned_CNN_Confusion_Matrix.png) in the specified folder (visuals_folder_aa).\n",
    "\n",
    "print(classification_report(y_test_encoded,y_pred_tcnn_labels)) - evaluates the performance of the Tuned CNN model on test data using Classification Report\n",
    "    \n",
    "update_score_card(y_test_encoded,y_pred_tcnn_labels,'Tuned_CNN_model') - calls the update score card method to update the score card with this model's score\n",
    "    \n",
    "#### K) Saving the models built\n",
    "       \n",
    "model_folder = \"C:/Users/nikde/Documents/UpGrad/intensityanalysis/models\"\n",
    "Defines the folder where models will be saved\n",
    "    \n",
    "joblib.dump(lr, os.path.join(model_folder, \"Initial_LR_model.pkl\"))\n",
    "joblib.dump(best_lr, os.path.join(model_folder, \"BestTuned_LR_model.pkl\"))\n",
    "joblib.dump(rf, os.path.join(model_folder, \"Initial_RFC_model.pkl\"))\n",
    "joblib.dump(best_rf, os.path.join(model_folder, \"BestTuned_RFC_model.pkl\"))\n",
    "joblib.dump(SVC, os.path.join(model_folder, \"Initial_SVC_model.pkl\"))\n",
    "joblib.dump(best_svc, os.path.join(model_folder, \"BestTuned_SVC_model.pkl\"))\n",
    "\n",
    "joblib.dump(alr, os.path.join(model_folder, \"Initial_Augmented_LR_model.pkl\"))\n",
    "joblib.dump(best_alr, os.path.join(model_folder, \"BestTuned_Augmented_LR_model.pkl\"))\n",
    "joblib.dump(arf, os.path.join(model_folder, \"Initial_Augmented_RFC_model.pkl\"))\n",
    "joblib.dump(aSVC, os.path.join(model_folder, \"Initial_Augmented_SVC_model.pkl\"))\n",
    "joblib.dump(best_asvc, os.path.join(model_folder, \"BestTuned_Augmented_SVC_model.pkl\"))\n",
    "\n",
    "joblib.dump(lstm_model1, os.path.join(model_folder, \"Initial_LSTM_model.pkl\"))\n",
    "joblib.dump(best_tuned_lstm_model, os.path.join(model_folder, \"Tuned_LSTM_model.pkl\"))\n",
    "joblib.dump(best_deep_lstm_model, os.path.join(model_folder, \"Deeper_Tuned_LSTM_model.pkl\"))\n",
    "\n",
    "joblib.dump(cnn_model, os.path.join(model_folder, \"Initial_CNN_model.pkl\"))\n",
    "joblib.dump(best_tcnnmodel, os.path.join(model_folder, \"Tuned_CNN_model.pkl\"))\n",
    "\n",
    "print(\"All models saved successfully!\")\n",
    "    \n",
    "This snippet saves each model with a different name in the models folder and prints a success message after saving them.\n",
    "    \n",
    "    \n",
    "#### L) Performance Comparison of all the models¶\n",
    "    \n",
    "sorted_score_card = score_card.sort_values(by='Accuracy Score', ascending=False).reset_index(drop=True)  \n",
    "- Sorts the score card generated by accuracy score in descending order\n",
    "\n",
    "final_score_card = sorted_score_card.head(10) - Stores only the top 10 models into final_score_card\n",
    "\n",
    "final_score_card - prints the final score card\n",
    "    \n",
    "file_path = os.path.join(\"C:/Users/nikde/Documents/UpGrad/intensityanalysis/data/processeddata\", \"best_performing_models.csv\")\n",
    "- Specifies the file path where you want to save the file\n",
    "\n",
    "final_score_card.to_csv(file_path, index=False)  \n",
    "Saves the final_score_card dataframe as a CSV file    \n",
    "Sets index=False to avoid saving the DataFrame index\n",
    "\n",
    "##### M) Deployment Plan and Future course\n",
    "\n",
    "After confirming the proper functioning of the project, I have uploaded the project folder with all the necessary files and folders to a newly created repository named (intensityanalysis) on my GitHub profile. \n",
    "\n",
    "Here is the link to the project repository : \n",
    "\n",
    "##### Deployment on Cloud Platforms\n",
    "\n",
    "To take this project to the next level, it can be deployed using various cloud platforms that specialize in machine learning services. Some of the most popular platforms include:\n",
    "\n",
    "AWS SageMaker: SageMaker allows for easy deployment of machine learning models. The platform provides pre-built infrastructure to train, host, and scale models, simplifying the process of deploying a model to a production environment.\n",
    "\n",
    "Azure Machine Learning: Azure ML provides a managed cloud environment where models can be trained and deployed. It supports integration with other Azure services, making it ideal for creating a complete data pipeline that handles both training and inference.\n",
    "\n",
    "Google Cloud Vertex AI: Vertex AI offers end-to-end machine learning workflows, enabling users to build, deploy, and scale models with ease. The platform also allows seamless integration with Google Cloud's data services for handling live data streams.\n",
    "\n",
    "Deploying the model on one of these platforms will enable real-time predictions using live or streaming data. By integrating the project with a cloud-based infrastructure, we can automate the entire machine learning pipeline, from data ingestion and preprocessing to model deployment and inference.\n",
    "\n",
    "Why Deploy to the Cloud?\n",
    "\n",
    "Scalability: Cloud platforms provide the ability to scale the model based on usage, automatically adjusting resources to handle increasing traffic or data volume.\n",
    "\n",
    "Real-Time Predictions: Deploying the model on the cloud allows for continuous predictions, processing new data as it arrives either in real time or at specified intervals.\n",
    "\n",
    "End-to-End Automation: Cloud deployment enables automation of the entire ML workflow, from data collection and processing to model training, evaluation, and prediction.\n",
    "\n",
    "Cost Efficiency: Using managed cloud services allows for pay-as-you-go pricing, ensuring that you only pay for the resources you use. Additionally, the infrastructure is managed by the cloud provider, reducing operational overhead.\n",
    "\n",
    "##### Future Course\n",
    "\n",
    "After deploying an NLP-based intensity prediction model on the cloud, it can be used in several ways and integrated into real-world systems across various industries. Here are potential future use cases and integration strategies for such a model:\n",
    "\n",
    "##### 1. Sentiment and Emotion Analysis Platforms\n",
    "Use Case: Cloud-based NLP models could be integrated into SaaS platforms offering sentiment and emotion analysis services to businesses.\n",
    "\n",
    "Integration: These models can be accessed via APIs by enterprises looking to analyze customer reviews, social media mentions, and other feedback. Companies could subscribe to these services to gain actionable insights on customer sentiment intensity, enabling them to adjust their marketing or customer service strategies dynamically.\n",
    "\n",
    "##### 2. Customer Support Automation\n",
    "Use Case: Integrating intensity prediction into customer support systems can help prioritize critical issues and improve response times for high-intensity (e.g., highly negative) cases.\n",
    "\n",
    "Integration: Cloud-based services could link the model to ticketing systems (like Zendesk or Freshdesk), where highly intense reviews or complaints are flagged for immediate attention. This ensures customer service teams focus on solving urgent issues, improving efficiency and customer satisfaction.\n",
    "\n",
    "##### 3. E-commerce and Product Review Filtering\n",
    "Use Case: E-commerce platforms can use the model to filter out irrelevant or spammy reviews based on extreme intensity levels or detect genuine feedback.\n",
    "\n",
    "Integration: This model can be incorporated into product review sections of online retailers like Amazon, where it helps classify, sort, and filter reviews. Intense positive or negative reviews can be highlighted to potential buyers, aiding decision-making and improving trust in the review system.\n",
    "\n",
    "##### 4. Social Media Monitoring and Crisis Management\n",
    "Use Case: Businesses and political entities can monitor public sentiment across social media and news platforms. Detecting high-intensity comments may help in identifying potential crises.\n",
    "\n",
    "Integration: The model can be deployed to monitor real-time Twitter, Facebook, or Instagram posts, and provide alerts when intense emotions (e.g., outrage or excitement) are detected, allowing companies to address issues or capitalize on positive trends.\n",
    "\n",
    "##### 5. Chatbots and Virtual Assistants\n",
    "Use Case: By integrating intensity prediction, conversational agents like chatbots can adjust their responses based on the emotional intensity of user inputs, offering a more empathetic experience.\n",
    "\n",
    "Integration: Cloud-hosted intensity prediction models can be plugged into AI-driven customer service chatbots (e.g., through platforms like Dialogflow or Rasa). When the model detects an intense emotional tone, the chatbot can escalate the conversation to a human agent or provide an appropriately sensitive response.\n",
    "\n",
    "##### 6. Mental Health and Emotional Well-being Applications\n",
    "Use Case: NLP models could be deployed in mental health applications to analyze user conversations or journal entries, detecting emotionally intense language that may indicate distress.\n",
    "\n",
    "Integration: Cloud-based models can be embedded into mental health platforms or mobile apps (like Woebot or Youper). The model can flag users who might need immediate intervention or recommend resources based on detected emotional intensity, supporting mental health professionals.\n",
    "\n",
    "##### 7. HR and Employee Engagement Platforms\n",
    "Use Case: Companies can use intensity prediction models to monitor employee feedback and engagement, especially in anonymous surveys and feedback forms.\n",
    "\n",
    "Integration: Deployed through cloud-based HR software, such as SAP SuccessFactors or BambooHR, the model can help HR departments identify employees experiencing high levels of frustration or dissatisfaction. This can lead to proactive interventions to improve workplace satisfaction.\n",
    "\n",
    "##### 8. Media and Content Recommendation Systems\n",
    "Use Case: Media platforms (e.g., Netflix or Spotify) can use intensity prediction models to personalize recommendations based on emotional intensity of previously watched or listened-to content.\n",
    "\n",
    "Integration: The model can be integrated with media platforms to analyze user reviews and feedback, allowing the recommendation engine to suggest content that matches a user's emotional state or preferences based on intensity.\n",
    "\n",
    "##### 9. Market Research and Consumer Insights\n",
    "Use Case: Marketing firms could use NLP models to analyze survey responses or focus group discussions for emotional intensity, helping them identify which aspects of a product evoke the strongest reactions.\n",
    "\n",
    "Integration: Cloud-based intensity prediction models can be built into market research tools that analyze open-ended survey questions. The model could generate insights on consumer feelings about a brand or product, feeding into reports or dashboards.\n",
    "\n",
    "##### 10. Real-time Feedback for Streaming and Broadcasting\n",
    "Use Case: Live broadcasters and streamers can monitor audience reactions in real-time, detecting highly intense feedback during important moments (e.g., in sports, gaming, or political broadcasts).\n",
    "\n",
    "Integration: This can be integrated into social media monitoring tools or live streaming platforms (like Twitch) where feedback is gathered and analyzed in real-time. When the model detects highly intense reactions, broadcasters can adjust content or respond to audience engagement quickly.\n",
    "\n",
    "##### Integration Strategies in Real-World Systems\n",
    "\n",
    "##### Cloud Hosting via API Services:\n",
    "\n",
    "Deploying the model as a microservice via cloud platforms like AWS, Azure, or GCP, accessible via RESTful APIs. This allows different systems to easily interact with the model for real-time or batch predictions.\n",
    "\n",
    "##### Data Pipelines:\n",
    "\n",
    "Integrating the model into data processing pipelines, where it becomes part of the end-to-end data workflow. Incoming customer reviews or social media posts are cleaned, preprocessed, passed to the model, and predictions are fed back to user dashboards.\n",
    "\n",
    "##### Plugging into CRMs:\n",
    "\n",
    "Customer Relationship Management (CRM) tools like Salesforce or HubSpot can embed this model into their feedback processing modules, enabling automated responses or escalation workflows based on intensity prediction.\n",
    "\n",
    "##### Mobile and Web Application Integration:\n",
    "\n",
    "Cloud-deployed models can be integrated into mobile apps or web interfaces via SDKs, enabling businesses to embed real-time intensity analysis directly into their platforms.\n",
    "\n",
    "##### Streaming Analytics:\n",
    "\n",
    "Using services like AWS Kinesis or Azure Stream Analytics, the intensity model can process streams of social media data or live customer feedback in real-time, generating dynamic dashboards or sending alerts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
